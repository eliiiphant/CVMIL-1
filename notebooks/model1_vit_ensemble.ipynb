{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0798aa39",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc37acd",
   "metadata": {},
   "source": [
    "Import all the required libraries for setting up data, model training, and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875f648",
   "metadata": {},
   "source": [
    "Set all necessary constants to be used in setting up data, model training, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1544b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VERSION = 6\n",
    "\n",
    "MANUAL_SEED = 27\n",
    "NUMBER_OF_CLASSES = 4\n",
    "\n",
    "RAW_DATA_DIR = \"../training_data\"\n",
    "\n",
    "EPOCHS = 30\n",
    "LAMBDA_WEIGHT = 0.5\n",
    "\n",
    "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
    "NOMRALIZE_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f66f4d",
   "metadata": {},
   "source": [
    "Set manual seeds in random libraries to ensure reproducibility of results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e15f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(MANUAL_SEED)\n",
    "np.random.seed(MANUAL_SEED)\n",
    "torch.manual_seed(MANUAL_SEED)\n",
    "torch.cuda.manual_seed_all(MANUAL_SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726aeb0f",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60038695",
   "metadata": {},
   "source": [
    "Start by loading in the dataset and determining the training set and validating set sizes. That is, 80% of the dataset will be used for training and the remaining 20% will be used for validation.\n",
    "\n",
    "Based on these determined sizes, the dataset will be split into their respective subsets randomly (with a pre-determined seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6a329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from directory into an iterable object\n",
    "base_dataset = datasets.ImageFolder(root=RAW_DATA_DIR)          \n",
    "\n",
    "# Determine subset sizes\n",
    "train_size = int(0.8*len(base_dataset))\n",
    "validation_size = len(base_dataset)-train_size\n",
    "\n",
    "# Split dataset into subsets randomly (with a pre-determined seed)\n",
    "generator = torch.Generator().manual_seed(MANUAL_SEED)\n",
    "train_dataset, validation_dataset = random_split(base_dataset, [train_size, validation_size], generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c066d3",
   "metadata": {},
   "source": [
    "To address class imbalance in the dataset, data augmentation will be applied to underrepresented classes to match the number of samples in the most represented class. Specifically, images from the *Cordana*, *Healthy*, and *Pestalotiopsis* classes will be used to create augmented images to achieve a more balanced class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241f480",
   "metadata": {},
   "source": [
    "Firstly, define transform functions that will be applied to images of the training dataset. These transformations include resizing, normalization, and data augmentation techniques, such as flipping, rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b6ab2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base transform function, will be applied to every image.\n",
    "transform_self_base = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "# Augmented transform function, will be applied in order to create new\n",
    "# images for underrepresented classes. \n",
    "transform_self_aug = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),          \n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.3,\n",
    "        contrast=0.3,\n",
    "        saturation=0.3,\n",
    "        hue=0.05\n",
    "    ),\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157c6e4",
   "metadata": {},
   "source": [
    "Next, define a subset processing function that will load original images (with base transform) for all classes but will also make use of the augmented transform function to produce augmented images **only for** underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31ca1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class_dataset(target_class: str, num_variants: int):\n",
    "    # Instantiate a subset of the training dataset based on given target_class.\n",
    "    class_index = train_dataset.dataset.class_to_idx[target_class]\n",
    "    indices = [i for i, (_, label) in enumerate(train_dataset.dataset.samples) if label == class_index]\n",
    "    train_subset = Subset(train_dataset.dataset, indices)\n",
    "\n",
    "    # Reverse the mapping to obtain classes using indices.\n",
    "    index_to_class = {v: k for k, v in train_dataset.dataset.class_to_idx.items()}\n",
    "\n",
    "    # For each class, retain original images but also produce augmented images\n",
    "    # if the class is underrepresented.\n",
    "    class_images = []\n",
    "    for i in range(len(train_subset)):\n",
    "        image, label = train_subset[i]\n",
    "        \n",
    "        # Append to class images list the original image with base transform.\n",
    "        base_image = transform_self_base(image)\n",
    "        class_images.append((base_image, label))\n",
    "\n",
    "        # Obtain class name.\n",
    "        class_name = index_to_class[label]\n",
    "\n",
    "        # If class is Sigatoka, skip augmented image production.\n",
    "        if class_name == \"sigatoka\":\n",
    "            continue\n",
    "\n",
    "        # Product (num_variants) augmented images using the augmented transform \n",
    "        # and append to class_images list.\n",
    "        for _ in range(num_variants):\n",
    "            augmented_image = transform_self_aug(image)\n",
    "            class_images.append((augmented_image, label))\n",
    "\n",
    "    return class_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2827c1",
   "metadata": {},
   "source": [
    "Finally, call this processing function to each of the classes. Collate the augmented images in *augmented_train_dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"sigatoka\", \"cordana\", \"healthy\", \"pestalotiopsis\"]\n",
    "augemented_train_dataset = []\n",
    "for class_name in classes:\n",
    "    class_dataset = load_class_dataset(class_name, 4)\n",
    "    augemented_train_dataset += class_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a9eef",
   "metadata": {},
   "source": [
    "### Training Proper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a6118",
   "metadata": {},
   "source": [
    "Now that the training set is ready and all classes have been balanced to have approximately equal sample sizes, model training can begin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b29fc",
   "metadata": {},
   "source": [
    "For this AI model, a pre-trained Vision Transformer (ViT) will be used, but with a twist: self-ensembling. Two ViTs will be deployed and trained, where one acts as the teacher and the other as the student. The teacher model is exposed to weakly augmented images, allowing it to remain stable and consistent. Meanwhile, the student model is trained on strongly augmented images, encouraging it to learn more robust features. This setup enables the teacher to guide and correct the student by enforcing consistency between their outputs.\n",
    "\n",
    "Additionally, cross-entropy loss is used to train the model on labeled data. To help the student learn from the teacher, a consistency loss (like Mean Squared Error) is also used to compare their predictions on augmented images. Using both losses together helps the student become more accurate and better at handling noisy or challenging inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039d522",
   "metadata": {},
   "source": [
    "To start this process, the weak (teacher) and strong (student) augmentation functions are firstly defined. For the weak augmentation, only a random horizontal flip is applied before converting the image to a Tensor and normalizing its values. Meanwhile, for the strong augmentation, a few transformations are defined.\n",
    "\n",
    "Additionally, a validation transform function is also defined as it will be used to convert validation set images to Tensors and normalize their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4f0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak transform for teacher training -> more stable and consistent output.\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=NORMALIZE_MEAN, \n",
    "        std=NOMRALIZE_STD\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Strong transform for student training -> learn robust features.\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5),\n",
    "    transforms.Normalize(\n",
    "        mean=NORMALIZE_MEAN, \n",
    "        std=NOMRALIZE_STD\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Convert validation set images to Tensors and normalize for processing later on.\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=NORMALIZE_MEAN, \n",
    "        std=NOMRALIZE_STD\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b9129",
   "metadata": {},
   "source": [
    "A DualTransformDataset is then defined to return both a weakly and a strongly augmented version of each base image.\n",
    "\n",
    "The new augmented training dataset, wrapped using the DualTransformDataset, is instantiated using the balanced dataset prepared in the Data Preprocessing section. Additionally, a new validation dataset is also created, now ensuring that all images are properly formatted and compatible for model evaluation.\n",
    "\n",
    "To enable batch processing during training and validation, a DataLoader is created using the augmented and validation training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b719f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DualTransformDataset object that returns weakly- and strongly-augmented\n",
    "# images, given the base image.\n",
    "class DualTransformDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, weak_transform, strong_transform):\n",
    "        self.dataset = dataset\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        return self.weak_transform(image), self.strong_transform(image), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Instantiate new datasets.\n",
    "augemented_train_dataset = DualTransformDataset(augemented_train_dataset, weak_transform, strong_transform)\n",
    "validation_dataset = [(validation_transform(image), label) for image, label in validation_dataset]\n",
    "\n",
    "# Instantiate loaders using datasets, for batch processing later on.\n",
    "train_loader = DataLoader(augemented_train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3454b",
   "metadata": {},
   "source": [
    "Class weights are then computed based on the distribution of labels in the training dataset to address any lingering class imbalances. The loss function is also defined using cross-entropy with label smoothing and the computed class weights to improve fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a8760b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Collect class labels and compute weights.\n",
    "targets = [label for _, _, label in augemented_train_dataset]\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(targets), y=targets)\n",
    "\n",
    "# Convert weights to Tensors and move to computing device.\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define loss function.\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1, weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f519584",
   "metadata": {},
   "source": [
    "Next, the student and teacher models are instantiated using a pretrained ViT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef9d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate student and teacher models using a pretrained Vision Transformer (ViT) model.\n",
    "student = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=NUMBER_OF_CLASSES)\n",
    "teacher = copy.deepcopy(student)\n",
    "\n",
    "# Move models to computing device.\n",
    "student = student.to(device)\n",
    "teacher = teacher.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b880c512",
   "metadata": {},
   "source": [
    "The *update_teacher()* function is also defined to update the teacher model’s weights using an exponential moving average (EMA) of the student model’s weights. The teacher is updated gradually to ensure it remains a stable target during training. The *ema_decay* parameter controls how much the teacher retains its previous values versus adopting the student’s current parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f440b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_teacher(student, teacher, ema_decay=0.99):\n",
    "    for student_param, teacher_param in zip(student.parameters(), teacher.parameters()):\n",
    "        teacher_param.data = ema_decay*teacher_param.data + (1-ema_decay)*student_param.data    # Exponential Moving Average (EMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10a32d3",
   "metadata": {},
   "source": [
    "The model training loop can now begin, after everything has been set.\n",
    "\n",
    "The training loop runs for a defined number of epochs, using the Adam optimizer to update the student model’s parameters. During each epoch, the student model is trained on strongly augmented images, while the teacher model remains in evaluation mode and predicts on weakly augmented images. Two losses are computed: cross-entropy loss for supervised learning, and a consistency loss (mean squared error) between student and teacher predictions. These are combined with a weighting factor.\n",
    "\n",
    "After each training step, the teacher model is updated using the Exponential Moving Average (EMA) of the student’s parameters. At the end of every epoch, the student model is evaluated on the validation set, and key metrics such as loss, accuracy, and macro F1 score are logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(student.parameters(), lr=3e-4, weight_decay=1e-4)  # Set up optimizer for the student model.\n",
    "best_val_acc = 0.0                                                              # Initialize best validation accuracy to 0.0%.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Set student to training mode and teacher to evaluation mode.\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    # Create progress bar for current epoch.\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for weak_img, strong_img, labels in loop:\n",
    "         # Move images and labels to the computing device.\n",
    "        weak_img, strong_img, labels = weak_img.to(device), strong_img.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass through teacher (no gradient computation).\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher(weak_img)\n",
    "\n",
    "        student_output = student(strong_img)                                    # Forward pass through student.\n",
    "        loss_ce = criterion(student_output, labels)                             # Compute cross-entropy loss on labeled data.\n",
    "        loss_consistency = mse_loss(student_output, teacher_output.detach())    # Compute consistency loss between student and teacher outputs.\n",
    "        loss = loss_ce + LAMBDA_WEIGHT * loss_consistency                       # Combine losses.\n",
    "\n",
    "        # Backpropagation and optimization step.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_teacher(student, teacher)                                        # Update teacher model via exponential moving average of student.\n",
    "\n",
    "        # Track training loss and accuracy.\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(student_output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update progress bar with current loss and accuracy.\n",
    "        loop.set_postfix(loss=running_loss / (total // 32), accuracy=100. * correct / total)\n",
    "    \n",
    "    # Evaluation phase, set student model to evaluation mode.\n",
    "    student.eval()\n",
    "\n",
    "    # Initialize evaluation variables.\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, labels in validation_loader:\n",
    "            image, labels = image.to(device), labels.to(device)\n",
    "\n",
    "            outputs = student(image)\n",
    "\n",
    "            # Compute loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy.\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Collect labels for F1 score.\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "    val_accuracy = 100. * val_correct / val_total\n",
    "\n",
    "    # Print validation accuracy and f1 score\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f} | Accuracy: {val_accuracy:.2f}% | F1 Score (macro): {f1:.4f}\")\n",
    "    \n",
    "    # Log accuracy and f1 score to text file\n",
    "    log_path = Path(\"../models/model1\") / f\"version_{TRAIN_VERSION}\" / \"training_log.txt\"\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(f\"Epoch {epoch+1}: Val Loss = {avg_val_loss:.4f}, Accuracy = {val_accuracy:.2f}%, F1 = {f1:.4f}\\n\")\n",
    "\n",
    "    # Save or overwrite file if current model (student and teacher) has better accuracy\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "\n",
    "        save_path = Path(\"../models/model1/\") / f\"version_{TRAIN_VERSION}\"\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        torch.save(student.state_dict(), save_path / \"student.pth\")\n",
    "        torch.save(teacher.state_dict(), save_path / \"teacher.pth\")\n",
    "        print(f\"Saved new best model at Epoch {epoch+1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
