{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88485067",
   "metadata": {},
   "source": [
    "## Importing Needed Libraries\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4359eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Data Preprocessing\n",
    "\n",
    "import torch\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Part 2: Creating the DCGAN Generator and Discriminator Classes\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Part 3: Training a DCGAN for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from numpy import cos, pi\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "\n",
    "# https://docs.pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "# https://pyimagesearch.com/2021/10/25/training-a-dcgan-in-pytorch/\n",
    "# https://medium.com/@manoharmanok/implementing-dcgan-in-pytorch-using-the-celeba-dataset-a-comprehensive-guide-660e6e8e29d2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d2cb2",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "411d5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "RAW_DATA_DIR = \"../training_data\"\n",
    "GAN_SIZE = (128, 128)\n",
    "CNN_SIZE = (224, 224)\n",
    "BANANA_CLASSES  = [\"cordana\", \"healthy\", \"pestalotiopsis\", \"sigatoka\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89358d3e",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12dfc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234d1bd",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8812e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_gan_b = transforms.Compose([\n",
    "    transforms.Resize(GAN_SIZE), # Resize for DCGAN\n",
    "    transforms.ToTensor(),       # To tensor\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5], \n",
    "        [0.5, 0.5, 0.5],\n",
    "    )  # Normalize to [-1, 1] for DCGAN\n",
    "])\n",
    "\n",
    "transform_gan_p = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(GAN_SIZE, scale = (0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomApply([ # maybe remove RandomApply for reproducibility?\n",
    "        transforms.ColorJitter(\n",
    "            brightness = 0.2, \n",
    "            contrast   = 0.2, \n",
    "            saturation = 0.2, \n",
    "            hue        = 0.05,\n",
    "        ),\n",
    "    ], p = 0.7),\n",
    "    transforms.RandomApply([ # maybe remove RandomApply for reproducibility?\n",
    "        transforms.RandomAffine(\n",
    "            degrees   = 10, \n",
    "            translate = (0.1, 0.1), \n",
    "            scale     = (0.9, 1.0), \n",
    "        ),\n",
    "    ], p = 0.7),\n",
    "])\n",
    "\n",
    "transform_cnn = transforms.Compose([\n",
    "    transforms.Resize(CNN_SIZE), # Resize for CNN\n",
    "    transforms.ToTensor(),       # To tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee0ae5",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53e02535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gan_data(batch_size = 32, workers = 4, target_class = None, num_variants = 10, seed = 42, directory = RAW_DATA_DIR):\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    # Load full dataset with base GAN transformations\n",
    "    dataset_gan = datasets.ImageFolder(root = directory, transform = transform_gan_b)\n",
    "\n",
    "    if target_class:\n",
    "        # Get class index from the class name\n",
    "        class_index = dataset_gan.class_to_idx[target_class]\n",
    "\n",
    "        # Filter indices where target matches\n",
    "        indices = [i for i, (_, label) in enumerate(dataset_gan.samples) if label == class_index]\n",
    "\n",
    "        # Wrap in a Subset\n",
    "        dataset_gan = Subset(dataset_gan, indices)\n",
    "\n",
    "        # Create a list to store augmented images\n",
    "        augmented_images = []\n",
    "\n",
    "        rng = random.Random(seed)\n",
    "\n",
    "        # Apply augmentations to each image in the loaded dataset\n",
    "        for i in range(len(dataset_gan)):\n",
    "            image, label = dataset_gan[i]\n",
    "\n",
    "            # Generate num_variants augmented versions of image\n",
    "            for _ in range(num_variants):\n",
    "                torch.manual_seed(rng.randint(0, 999999))\n",
    "        \n",
    "                augmented_image = transform_gan_p(image)\n",
    "\n",
    "                augmented_images.append((augmented_image, label))\n",
    "\n",
    "        # Create new dataset with augmented images\n",
    "        final_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.stack([image for image, _ in augmented_images]),  # Stack all augmented images\n",
    "            torch.tensor([label for _, label in augmented_images])  # Stack all labels\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        final_dataset = dataset_gan\n",
    "\n",
    "    # Create DataLoader for the GAN data\n",
    "    dataloader_gan = DataLoader(final_dataset, batch_size = batch_size, shuffle = True, num_workers = workers, generator = generator)\n",
    "\n",
    "    return dataloader_gan\n",
    "\n",
    "def load_cnn_data(batch_size = 32, workers = 4):\n",
    "    # Load dataset with CNN transformations\n",
    "    dataset_cnn = datasets.ImageFolder(root=RAW_DATA_DIR, transform = transform_cnn)\n",
    "    \n",
    "    # Create DataLoader for the CNN data\n",
    "    dataloader_cnn = DataLoader(dataset_cnn, batch_size=batch_size, shuffle = True, num_workers = workers)\n",
    "\n",
    "    return dataloader_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2af1f2",
   "metadata": {},
   "source": [
    "## Part 2: Creating the DCGAN Generator and Discriminator Classes\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33db2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "SEED_NUM = 42            # Seed number for reproducibility\n",
    "\n",
    "BATCH_SIZE = 128         # Number of images per training batch\n",
    "\n",
    "INPUT_DIMENSION = 100    # Dimensionality of the generator input\n",
    "\n",
    "NC = 3                   # Number of channels in the training images\n",
    "\n",
    "NGF = 64                 # Base number of feature maps in the Generator\n",
    "\n",
    "NDF = 64                 # Base number of feature maps in the Discriminator\n",
    "\n",
    "EPOCHS = 200             # Number of training epochs\n",
    "\n",
    "CHECKPOINT = 5           # Checkpoint number for model saving\n",
    "\n",
    "LEARNING_RATE_G = 0.0002 # Learning rate for the Generator\n",
    "\n",
    "LEARNING_RATE_D = 0.0001 # Learning rate for the Discriminator\n",
    "\n",
    "BETA1 = 0.5              # Beta1 value for the Adam optimizer to help stabilize DCGAN training\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "NGPU = 1  # Number of GPUs to use (0 means CPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6d25128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Generator network composed of a stack of transposed conv blocks\n",
    "        self.main = nn.Sequential(\n",
    "            self._block(INPUT_DIMENSION, NGF * 16, 4, 1, 0, bias = False),  # First layer: latent vector -> feature map\n",
    "            self._block(NGF * 16, NGF * 8, 4, 2, 1, bias = False),          # Upsample to 8 x 8\n",
    "            self._block(NGF * 8, NGF * 4, 4, 2, 1, bias = False),           # Upsample to 16 x 16\n",
    "            self._block(NGF * 4, NGF * 2, 4, 2, 1, bias = False),           # Upsample to 32 x 32\n",
    "            self._block(NGF * 2, NGF, 4, 2, 1, bias = False),               # Upsample to 64 x 64\n",
    "\n",
    "            nn.ConvTranspose2d(NGF, NC, 4, 2, 1, bias = False),             # Final upsample to 128 x 128 with RGB output\n",
    "            nn.Tanh()                                                       # Output pixel values in [-1, 1]\n",
    "        )\n",
    "\n",
    "    # Helper function to define a generator block:\n",
    "\n",
    "    # ConvTranspose2d -> InstanceNorm2d -> ReLU -> Dropout\n",
    "\n",
    "    def _block(self, i_channels, o_channels, kernel_size, stride, padding, bias):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                i_channels, \n",
    "                o_channels, \n",
    "                kernel_size, \n",
    "                stride, \n",
    "                padding, \n",
    "                bias = bias),\n",
    "            nn.InstanceNorm2d(o_channels, affine = True),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3) # Dropout to help regularize on small data\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7581479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Discriminator network composed of downsampling conv blocks\n",
    "        self.main = nn.Sequential(\n",
    "            self._block(NC, NDF, 4, 2, 1, bias = False, use_instanceNorm2d = False), # First block: no InstanceNorm2d\n",
    "            self._block(NDF, NDF *  2, 4, 2, 1, bias = False),                       # Downsample to 32 x 32\n",
    "            self._block(NDF * 2, NDF *  4, 4, 2, 1, bias = False),                   # Downsample to 16 x 16\n",
    "            self._block(NDF * 4, NDF *  8, 4, 2, 1, bias = False),                   # Downsample to 8 x 8\n",
    "            self._block(NDF * 8, NDF * 16, 4, 2, 1, bias = False),                   # Downsample to 4 x 4\n",
    "\n",
    "            nn.Conv2d(NDF * 16, 1, 4, 1, 0, bias = False),                           # Final layer: reduce to 1 x 1\n",
    "        )\n",
    "\n",
    "    # Helper function to define a discriminator block:\n",
    "\n",
    "    # Conv2d -> (optional) InstanceNorm2d -> LeakyReLU\n",
    "\n",
    "    def _block(self, i_channels, o_channels, kernel_size, stride, padding, bias, use_instanceNorm2d = True):\n",
    "        layers = [nn.Conv2d(\n",
    "            i_channels, \n",
    "            o_channels, \n",
    "            kernel_size, \n",
    "            stride, \n",
    "            padding, \n",
    "            bias = bias)]\n",
    "        \n",
    "        if use_instanceNorm2d:\n",
    "            layers.append(nn.InstanceNorm2d(o_channels, affine = True))\n",
    "        \n",
    "        layers.append(nn.LeakyReLU(0.2, inplace = True))\n",
    "        layers.append(nn.Dropout(0.3)) # Dropout to help regularize on small data\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb85cf",
   "metadata": {},
   "source": [
    "## Part 3: Training a DCGAN for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "014a3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find(\"InstanceNorm\") != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99b790",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20fa7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_OUTPUT_DIRECTORY_TEST = \"../model2/gan_test\" # for debugging while training\n",
    "\n",
    "def prepare_output_directory(resume = False):\n",
    "    for cls in [\"base\"] + BANANA_CLASSES:\n",
    "        if cls != \"sigatoka\":\n",
    "            full_path = Path(GAN_OUTPUT_DIRECTORY_TEST) / cls\n",
    "\n",
    "            # Only remove and recreate the directory if not resuming\n",
    "            if not resume and full_path.exists():\n",
    "                shutil.rmtree(full_path)\n",
    "\n",
    "            full_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# prepare_output_directory(resume = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff876c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dcgan(target_class = None, resume = False, checkpoint_path = None, balanced_path = None):\n",
    "    # Set random seed for reproducibility\n",
    "    set_seed(SEED_NUM)\n",
    "\n",
    "    # Load dataset and define save path\n",
    "    if balanced_path:  \n",
    "        save_path = \"../model2/final_dcgan\"\n",
    "\n",
    "        dataloader = load_gan_data(batch_size = BATCH_SIZE, directory = balanced_path)\n",
    "\n",
    "    else:\n",
    "        save_path = GAN_OUTPUT_DIRECTORY_TEST\n",
    "\n",
    "        if target_class:\n",
    "            dataloader = load_gan_data(batch_size = BATCH_SIZE, target_class = target_class)\n",
    "\n",
    "        else:\n",
    "            dataloader = load_gan_data(batch_size = BATCH_SIZE)\n",
    "\n",
    "    # Initialize Generator and Discriminator\n",
    "    netG = Generator(ngpu = NGPU).to(DEVICE)\n",
    "    netD = Discriminator(ngpu = NGPU).to(DEVICE)\n",
    "\n",
    "    # Handle multi-GPU setup if applicable\n",
    "    if (DEVICE.type == \"cuda\") and (NGPU > 1):\n",
    "        netG = nn.DataParallel(netG, list(range(NGPU)))\n",
    "        netD = nn.DataParallel(netD, list(range(NGPU)))\n",
    "\n",
    "    # Fixed noise for generating sample outputs and tracking progress during training\n",
    "    fixed_generator = torch.Generator(device=DEVICE).manual_seed(SEED_NUM)\n",
    "\n",
    "    fixed_noise = torch.randn(64, INPUT_DIMENSION, 1, 1, device = DEVICE, generator = fixed_generator)\n",
    "\n",
    "    # Optimizers for Generator and Discriminator\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr = LEARNING_RATE_D, betas = (BETA1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr = LEARNING_RATE_G, betas = (BETA1, 0.999))\n",
    "\n",
    "    # Schedulers for optimizers\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerD, T_max = EPOCHS, eta_min = 1e-6)\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerG, T_max = EPOCHS, eta_min = 1e-6)\n",
    "\n",
    "    # Default starting epoch\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Loads checkpoint dcgan if provided \n",
    "\n",
    "    # Case 1: train base dcgan from a checkpoint\n",
    "    # Case 2: train per-class dcgan from scratch with base dcgan weights as a starting point\n",
    "    # Case 3: train per-class dcgan from a checkpoint\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location = DEVICE)\n",
    "        netG.load_state_dict(checkpoint[\"netG\"])\n",
    "        netD.load_state_dict(checkpoint[\"netD\"])\n",
    "\n",
    "        # Loads checkpoint optimizers if resuming training\n",
    "        if resume:\n",
    "            optimizerG.load_state_dict(checkpoint[\"optimizerG\"])\n",
    "            optimizerD.load_state_dict(checkpoint[\"optimizerD\"])\n",
    "            start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    # Fresh start\n",
    "    else:\n",
    "        netG.apply(initialize_weights)\n",
    "        netD.apply(initialize_weights)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Labels for real and fake images\n",
    "    real_label = 0.9 # Slightly less than 1\n",
    "    fake_label = 0.1 # Slightly more than 0\n",
    "\n",
    "    # Actual training\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        # Calculate noise magnitude decay\n",
    "        noise_magnitude = 0.1 * 0.5 * (1 + cos(pi * epoch / EPOCHS)) # 0.1 = maximum noise magnitude\n",
    "    \n",
    "        for i, (real_images, _) in enumerate(dataloader): # Iterate through batches in the dataset\n",
    "            # 1. Update Discriminator: \n",
    "            #    maximize log(D(x)) + log(1 - D(G(z)))\n",
    "\n",
    "            # 1.A. Train Discriminator on real images\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # Format real batch\n",
    "            real_images = real_images.to(DEVICE)\n",
    "\n",
    "            # Well, train the Discriminator on noisy real images\n",
    "            noise = torch.randn_like(real_images) * noise_magnitude # noise magnitude decays\n",
    "\n",
    "            noisy_real_images = real_images + noise\n",
    "\n",
    "            size = real_images.size(0)\n",
    "\n",
    "            label = torch.full((size,), real_label, dtype = torch.float, device = DEVICE)\n",
    "\n",
    "            # Forward pass noisy real images through Discriminator\n",
    "            output = netD(noisy_real_images).view(-1)\n",
    "\n",
    "            # Calculate Discriminator loss for noisy real images\n",
    "            errD_real = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for noisy real images\n",
    "            errD_real.backward()\n",
    "\n",
    "            # Mean output for noisy real images\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # 1.B. Train Discriminator on batch of all fake images\n",
    "\n",
    "            # Generate batch of latent vectors\n",
    "            generator = torch.Generator(device=DEVICE).manual_seed(SEED_NUM + epoch)\n",
    "\n",
    "            noise = torch.randn(size, INPUT_DIMENSION, 1, 1, device = DEVICE, generator = generator)\n",
    "\n",
    "            # Generate fake images with Generator\n",
    "            fake = netG(noise)\n",
    "\n",
    "            # Classify fake images with Discriminator\n",
    "            label.fill_(fake_label)\n",
    "\n",
    "            # Forward pass fake images through Discriminator\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "\n",
    "            # Calculate Discriminator loss for fake images\n",
    "            errD_fake = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for fake images\n",
    "            errD_fake.backward()\n",
    "\n",
    "            # Clip Discriminator gradients for stability\n",
    "            torch.nn.utils.clip_grad_norm_(netD.parameters(), max_norm = 1.0)\n",
    "\n",
    "            # Mean output for fake images\n",
    "            D_G_z1 = output.mean().item()\n",
    "\n",
    "            # Compute total Discriminator error = real error + fake error\n",
    "            errD = errD_real + errD_fake\n",
    "\n",
    "            # Finally update Discriminator\n",
    "            optimizerD.step()\n",
    "\n",
    "            # 2. Update Generator: \n",
    "            #    maximize log(D(G(z)))\n",
    "\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for Generator cost\n",
    "\n",
    "            # Pass fake images through Discriminator\n",
    "            output = netD(fake).view(-1)\n",
    "\n",
    "            # Calculate Generator loss based on Discriminator's output\n",
    "            errG = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for Generator\n",
    "            errG.backward()\n",
    "\n",
    "            # Mean output for fake images after Generator update\n",
    "            D_G_z2 = output.mean().item()\n",
    "\n",
    "            # Finally update Generator\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Debugging: Print losses and monitor training progress\n",
    "\n",
    "            if i in [0, len(dataloader)//2]:\n",
    "                print(\n",
    "                  f\"Epoch [{epoch}/{EPOCHS}] Batch {i}/{len(dataloader)} \\\n",
    "                    Loss D: {errD.item():.4f}, loss G: {errG.item():.4f} \\\n",
    "                    D(x): {D_x:.4f}, \\\n",
    "                    D(G(z))_real: {D_G_z1:.4f}, D(G(z))_fake: {D_G_z2:.4f}\"\n",
    "                )\n",
    "\n",
    "        # Step learning rate schedulers\n",
    "        schedulerD.step()\n",
    "        schedulerG.step()\n",
    "\n",
    "        if epoch % CHECKPOINT == 0:\n",
    "            fake_images = netG(fixed_noise).detach()\n",
    "\n",
    "            if target_class:\n",
    "                path = f\"{save_path}/{target_class}\"\n",
    "            \n",
    "            else:\n",
    "                # root folder of save path, else /base if base dcgan\n",
    "                path = f\"{save_path}{\"\" if balanced_path else \"/base\"}\"\n",
    "\n",
    "            # Save images Generator could produce during checkpoints\n",
    "            save_image(\n",
    "                fake_images,\n",
    "                os.path.join(path, f\"sample_epoch_{epoch}.png\"),\n",
    "                normalize = True\n",
    "            )\n",
    "\n",
    "            # Save model version\n",
    "            save_dict = {\n",
    "                \"epoch\": epoch,\n",
    "                \"netG\": netG.state_dict(),\n",
    "                \"netD\": netD.state_dict(),\n",
    "                \"optimizerG\": optimizerG.state_dict(),\n",
    "                \"optimizerD\": optimizerD.state_dict(),\n",
    "            }\n",
    "\n",
    "            torch.save(save_dict, os.path.join(path, f\"checkpoint_epoch_{epoch}.pth\"))\n",
    "\n",
    "    torch.save({\n",
    "        \"epoch\": EPOCHS,\n",
    "        \"netG\": netG.state_dict(),\n",
    "        \"netD\": netD.state_dict(),\n",
    "        \"optimizerG\": optimizerG.state_dict(),\n",
    "        \"optimizerD\": optimizerD.state_dict(),\n",
    "    }, os.path.join(f\"{save_path}\", f\"{target_class if target_class else (\"final\" if balanced_path else \"base\")}_dcgan_final.pth\"))\n",
    "\n",
    "    return netG, netD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be787ca7",
   "metadata": {},
   "source": [
    "### Part 3.1: Training a Base DCGAN\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "319e07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Base DCGAN\n",
    "\n",
    "# trained_generator_base, trained_discriminator_base = train_dcgan(target_class = None, resume = False, checkpoint_path = None)\n",
    "\n",
    "# trained_generator_base, trained_discriminator_base = train_dcgan(target_class = None, resume = True, checkpoint_path = '../model2/gan_test/base/checkpoint_epoch_10.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6e100",
   "metadata": {},
   "source": [
    "### Part 3.2: Training a DCGAN Per Class\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09fd2ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/12                     Loss D: 2.4340, loss G: 1.8551                     D(x): 0.1693,                     D(G(z))_real: 0.6190, D(G(z))_fake: -1.5289\n",
      "Epoch [0/10] Batch 6/12                     Loss D: 1.5898, loss G: 1.8516                     D(x): 0.6599,                     D(G(z))_real: -0.9129, D(G(z))_fake: -1.6045\n",
      "Epoch [1/10] Batch 0/12                     Loss D: 1.8382, loss G: 2.4138                     D(x): 1.0232,                     D(G(z))_real: -0.1530, D(G(z))_fake: -2.3428\n",
      "Epoch [1/10] Batch 6/12                     Loss D: 1.7139, loss G: 1.6042                     D(x): 0.4293,                     D(G(z))_real: -1.3681, D(G(z))_fake: -1.2085\n",
      "Epoch [2/10] Batch 0/12                     Loss D: 1.6814, loss G: 1.5338                     D(x): 0.3853,                     D(G(z))_real: -1.2421, D(G(z))_fake: -1.0678\n",
      "Epoch [2/10] Batch 6/12                     Loss D: 1.6585, loss G: 1.9045                     D(x): 0.8042,                     D(G(z))_real: -0.8743, D(G(z))_fake: -1.6181\n",
      "Epoch [3/10] Batch 0/12                     Loss D: 1.5657, loss G: 3.0474                     D(x): 1.5761,                     D(G(z))_real: -0.6563, D(G(z))_fake: -3.1683\n",
      "Epoch [3/10] Batch 6/12                     Loss D: 1.7285, loss G: 1.5347                     D(x): 0.6119,                     D(G(z))_real: -0.6448, D(G(z))_fake: -0.9967\n",
      "Epoch [4/10] Batch 0/12                     Loss D: 1.5063, loss G: 2.3694                     D(x): 1.0601,                     D(G(z))_real: -0.9653, D(G(z))_fake: -2.1939\n",
      "Epoch [4/10] Batch 6/12                     Loss D: 1.2937, loss G: 1.8504                     D(x): 1.5552,                     D(G(z))_real: -1.4793, D(G(z))_fake: -1.5031\n",
      "Epoch [5/10] Batch 0/12                     Loss D: 1.1914, loss G: 2.3395                     D(x): 1.7751,                     D(G(z))_real: -2.4596, D(G(z))_fake: -2.2525\n",
      "Epoch [5/10] Batch 6/12                     Loss D: 1.0883, loss G: 3.0468                     D(x): 2.7542,                     D(G(z))_real: -1.7875, D(G(z))_fake: -3.1907\n",
      "Epoch [6/10] Batch 0/12                     Loss D: 1.1739, loss G: 2.5556                     D(x): 2.6318,                     D(G(z))_real: -1.8013, D(G(z))_fake: -2.5866\n",
      "Epoch [6/10] Batch 6/12                     Loss D: 1.0738, loss G: 2.4376                     D(x): 2.4428,                     D(G(z))_real: -2.2702, D(G(z))_fake: -2.4470\n",
      "Epoch [7/10] Batch 0/12                     Loss D: 1.0562, loss G: 2.6071                     D(x): 2.3742,                     D(G(z))_real: -2.6205, D(G(z))_fake: -2.6101\n",
      "Epoch [7/10] Batch 6/12                     Loss D: 1.1175, loss G: 2.4833                     D(x): 2.5512,                     D(G(z))_real: -1.8575, D(G(z))_fake: -2.4956\n",
      "Epoch [8/10] Batch 0/12                     Loss D: 1.0016, loss G: 2.4611                     D(x): 2.6756,                     D(G(z))_real: -2.4936, D(G(z))_fake: -2.4809\n",
      "Epoch [8/10] Batch 6/12                     Loss D: 1.1024, loss G: 2.8979                     D(x): 2.2077,                     D(G(z))_real: -2.4554, D(G(z))_fake: -3.0627\n",
      "Epoch [9/10] Batch 0/12                     Loss D: 0.9628, loss G: 2.4126                     D(x): 2.5367,                     D(G(z))_real: -2.5342, D(G(z))_fake: -2.4276\n",
      "Epoch [9/10] Batch 6/12                     Loss D: 1.0349, loss G: 2.6368                     D(x): 2.7162,                     D(G(z))_real: -2.3326, D(G(z))_fake: -2.6946\n"
     ]
    }
   ],
   "source": [
    "# Train DCGAN for \"cordana\" class\n",
    "# trained_generator_cordana, _ = train_dcgan(target_class = \"cordana\")\n",
    "\n",
    "# trained_generator_cordana, _ = train_dcgan(target_class = \"cordana\", resume = True, checkpoint_path = \"../model2/gan_test/cordana/checkpoint_epoch_<insert>.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffe98212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/10                     Loss D: 2.2527, loss G: 2.2237                     D(x): 0.7456,                     D(G(z))_real: 0.7270, D(G(z))_fake: -2.1312\n",
      "Epoch [0/10] Batch 5/10                     Loss D: 1.4573, loss G: 1.6809                     D(x): 1.1316,                     D(G(z))_real: -1.2643, D(G(z))_fake: -1.2100\n",
      "Epoch [1/10] Batch 0/10                     Loss D: 1.5428, loss G: 2.1511                     D(x): 1.0789,                     D(G(z))_real: -1.1019, D(G(z))_fake: -1.9966\n",
      "Epoch [1/10] Batch 5/10                     Loss D: 1.4696, loss G: 1.7577                     D(x): 1.4952,                     D(G(z))_real: -1.1386, D(G(z))_fake: -1.3757\n",
      "Epoch [2/10] Batch 0/10                     Loss D: 1.8582, loss G: 1.6745                     D(x): 0.6707,                     D(G(z))_real: -1.0354, D(G(z))_fake: -1.2062\n",
      "Epoch [2/10] Batch 5/10                     Loss D: 1.7404, loss G: 1.5236                     D(x): 0.9865,                     D(G(z))_real: -0.9365, D(G(z))_fake: -1.0959\n",
      "Epoch [3/10] Batch 0/10                     Loss D: 1.6721, loss G: 2.2647                     D(x): 1.5459,                     D(G(z))_real: -0.6143, D(G(z))_fake: -2.1806\n",
      "Epoch [3/10] Batch 5/10                     Loss D: 1.5813, loss G: 1.8923                     D(x): 1.6466,                     D(G(z))_real: -0.6455, D(G(z))_fake: -1.6605\n",
      "Epoch [4/10] Batch 0/10                     Loss D: 1.6249, loss G: 1.5856                     D(x): 0.8220,                     D(G(z))_real: -1.3477, D(G(z))_fake: -1.1309\n",
      "Epoch [4/10] Batch 5/10                     Loss D: 1.6032, loss G: 1.8446                     D(x): 1.1719,                     D(G(z))_real: -0.7338, D(G(z))_fake: -1.5311\n",
      "Epoch [5/10] Batch 0/10                     Loss D: 1.3608, loss G: 2.0105                     D(x): 1.8331,                     D(G(z))_real: -1.0846, D(G(z))_fake: -1.8049\n",
      "Epoch [5/10] Batch 5/10                     Loss D: 1.5601, loss G: 1.9583                     D(x): 1.0821,                     D(G(z))_real: -1.2561, D(G(z))_fake: -1.7283\n",
      "Epoch [6/10] Batch 0/10                     Loss D: 1.5863, loss G: 1.7923                     D(x): 1.4964,                     D(G(z))_real: -0.7029, D(G(z))_fake: -1.3733\n",
      "Epoch [6/10] Batch 5/10                     Loss D: 1.6147, loss G: 1.7382                     D(x): 1.1467,                     D(G(z))_real: -0.8142, D(G(z))_fake: -1.3592\n",
      "Epoch [7/10] Batch 0/10                     Loss D: 1.5579, loss G: 1.9129                     D(x): 1.0232,                     D(G(z))_real: -1.2591, D(G(z))_fake: -1.5986\n",
      "Epoch [7/10] Batch 5/10                     Loss D: 1.4562, loss G: 1.7015                     D(x): 1.3641,                     D(G(z))_real: -1.2414, D(G(z))_fake: -1.2988\n",
      "Epoch [8/10] Batch 0/10                     Loss D: 1.4282, loss G: 1.8588                     D(x): 1.4298,                     D(G(z))_real: -1.2925, D(G(z))_fake: -1.5117\n",
      "Epoch [8/10] Batch 5/10                     Loss D: 1.4674, loss G: 1.7872                     D(x): 1.6721,                     D(G(z))_real: -1.4381, D(G(z))_fake: -1.4325\n",
      "Epoch [9/10] Batch 0/10                     Loss D: 1.3177, loss G: 1.7243                     D(x): 1.7197,                     D(G(z))_real: -1.3996, D(G(z))_fake: -1.3826\n",
      "Epoch [9/10] Batch 5/10                     Loss D: 1.2373, loss G: 1.8576                     D(x): 1.8636,                     D(G(z))_real: -1.4794, D(G(z))_fake: -1.5499\n"
     ]
    }
   ],
   "source": [
    "# Train DCGAN for \"healthy\" class\n",
    "# trained_generator_healthy, _ = train_dcgan(target_class = \"healthy\")\n",
    "\n",
    "# trained_generator_healthy, _ = train_dcgan(target_class = \"healthy\", resume = True, checkpoint_path = \"../model2/gan_test/healthy/checkpoint_epoch_<insert>.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e819ef44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/13                     Loss D: 2.3619, loss G: 1.0048                     D(x): -0.9363,                     D(G(z))_real: -0.4710, D(G(z))_fake: 0.2869\n",
      "Epoch [0/10] Batch 6/13                     Loss D: 1.5460, loss G: 1.6357                     D(x): 0.6044,                     D(G(z))_real: -1.4580, D(G(z))_fake: -1.2098\n",
      "Epoch [1/10] Batch 0/13                     Loss D: 1.6684, loss G: 2.1246                     D(x): 1.7808,                     D(G(z))_real: -0.3231, D(G(z))_fake: -1.9518\n",
      "Epoch [1/10] Batch 6/13                     Loss D: 1.8293, loss G: 1.7620                     D(x): 0.7517,                     D(G(z))_real: -0.6983, D(G(z))_fake: -1.3806\n",
      "Epoch [2/10] Batch 0/13                     Loss D: 1.7056, loss G: 1.8361                     D(x): 0.9674,                     D(G(z))_real: -0.7404, D(G(z))_fake: -1.4852\n",
      "Epoch [2/10] Batch 6/13                     Loss D: 1.9702, loss G: 1.6130                     D(x): 0.8530,                     D(G(z))_real: -0.2414, D(G(z))_fake: -1.2016\n",
      "Epoch [3/10] Batch 0/13                     Loss D: 1.6959, loss G: 1.6048                     D(x): 0.5426,                     D(G(z))_real: -1.0843, D(G(z))_fake: -1.1155\n",
      "Epoch [3/10] Batch 6/13                     Loss D: 1.7133, loss G: 1.8542                     D(x): 0.7645,                     D(G(z))_real: -0.6806, D(G(z))_fake: -1.5188\n",
      "Epoch [4/10] Batch 0/13                     Loss D: 1.6980, loss G: 1.5301                     D(x): 0.2757,                     D(G(z))_real: -1.5532, D(G(z))_fake: -1.0561\n",
      "Epoch [4/10] Batch 6/13                     Loss D: 1.5620, loss G: 2.3258                     D(x): 1.0995,                     D(G(z))_real: -0.9499, D(G(z))_fake: -2.2530\n",
      "Epoch [5/10] Batch 0/13                     Loss D: 1.5452, loss G: 1.9935                     D(x): 0.9902,                     D(G(z))_real: -1.1940, D(G(z))_fake: -1.7531\n",
      "Epoch [5/10] Batch 6/13                     Loss D: 1.4680, loss G: 2.2275                     D(x): 0.9145,                     D(G(z))_real: -1.1735, D(G(z))_fake: -2.1565\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Seth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\ImageFile.py:643\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(im, fp, tile, bufsize)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     fh = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfileno\u001b[49m()\n\u001b[32m    644\u001b[39m     fp.flush()\n",
      "\u001b[31mAttributeError\u001b[39m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train DCGAN for \"pestalotiopsis\" class\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trained_generator_pestalotiopsis, _ = \u001b[43mtrain_dcgan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpestalotiopsis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# trained_generator_pestalotiopsis, _ = train_dcgan_per_class(target_class = \"pestalotiopsis\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 180\u001b[39m, in \u001b[36mtrain_dcgan\u001b[39m\u001b[34m(target_class, resume, checkpoint_path)\u001b[39m\n\u001b[32m    177\u001b[39m     path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGAN_OUTPUT_DIRECTORY_TEST\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/base\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Save images Generator could produce during checkpoints\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfake_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msample_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.png\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    184\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Save model version\u001b[39;00m\n\u001b[32m    187\u001b[39m save_dict = {\n\u001b[32m    188\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m: epoch,\n\u001b[32m    189\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnetG\u001b[39m\u001b[33m\"\u001b[39m: netG.state_dict(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    192\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moptimizerD\u001b[39m\u001b[33m\"\u001b[39m: optimizerD.state_dict(),\n\u001b[32m    193\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Seth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Seth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\utils.py:151\u001b[39m, in \u001b[36msave_image\u001b[39m\u001b[34m(tensor, fp, format, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m ndarr = grid.mul(\u001b[32m255\u001b[39m).add_(\u001b[32m0.5\u001b[39m).clamp_(\u001b[32m0\u001b[39m, \u001b[32m255\u001b[39m).permute(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, torch.uint8).numpy()\n\u001b[32m    150\u001b[39m im = Image.fromarray(ndarr)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Seth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\Image.py:2581\u001b[39m, in \u001b[36mImage.save\u001b[39m\u001b[34m(self, fp, format, **params)\u001b[39m\n\u001b[32m   2578\u001b[39m     fp = cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2582\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   2583\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Seth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\PngImagePlugin.py:1492\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(im, fp, filename, chunk, save_all)\u001b[39m\n\u001b[32m   1488\u001b[39m     single_im = _write_multiple_frames(\n\u001b[32m   1489\u001b[39m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[32m   1490\u001b[39m     )\n\u001b[32m   1491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m single_im:\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m     \u001b[43mImageFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIO\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1495\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mImageFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Tile\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[32m   1499\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info.chunks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Seth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\ImageFile.py:647\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(im, fp, tile, bufsize)\u001b[39m\n\u001b[32m    645\u001b[39m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io.UnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[33m\"\u001b[39m\u001b[33mflush\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    649\u001b[39m     fp.flush()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Seth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\ImageFile.py:674\u001b[39m, in \u001b[36m_encode_tile\u001b[39m\u001b[34m(im, fp, tile, bufsize, fh, exc)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    673\u001b[39m     errcode, data = encoder.encode(bufsize)[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    675\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errcode:\n\u001b[32m    676\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Seth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\PngImagePlugin.py:1136\u001b[39m, in \u001b[36m_idat.write\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: \u001b[38;5;28mbytes\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIDAT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Seth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\PngImagePlugin.py:1123\u001b[39m, in \u001b[36mputchunk\u001b[39m\u001b[34m(fp, cid, *data)\u001b[39m\n\u001b[32m   1120\u001b[39m byte_data = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(data)\n\u001b[32m   1122\u001b[39m fp.write(o32(\u001b[38;5;28mlen\u001b[39m(byte_data)) + cid)\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1124\u001b[39m crc = _crc32(byte_data, _crc32(cid))\n\u001b[32m   1125\u001b[39m fp.write(o32(crc))\n",
      "\u001b[31mOSError\u001b[39m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# Train DCGAN for \"pestalotiopsis\" class\n",
    "# trained_generator_pestalotiopsis, _ = train_dcgan(target_class = \"pestalotiopsis\")\n",
    "\n",
    "# trained_generator_pestalotiopsis, _ = train_dcgan(target_class = \"pestalotiopsis\", resume = True, checkpoint_path = \"../model2/gan_test/pestalotiopsis/checkpoint_epoch_<insert>.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3e960",
   "metadata": {},
   "source": [
    "## Part 4: Generating Images for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_images(dcgan_generator, amount_to_generate, class_label, output_directory):\n",
    "\n",
    "    # Set the generator to evaluation mode to disable Dropout and InstanceNorm2d updates\n",
    "    dcgan_generator.eval()\n",
    "\n",
    "    # Construct the path to the class-specific output directory\n",
    "    class_output_directory = os.path.join(output_directory, class_label)\n",
    "\n",
    "    # Create the output directory if it does not exist just in case\n",
    "    os.makedirs(class_output_directory, exist_ok = True)\n",
    "\n",
    "    # Disable gradient computation for efficiency during inference\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, amount_to_generate, 16): # Batches of 16\n",
    "            batch_size = min(16, amount_to_generate - i) # Adjusts batch size if near the end of generation\n",
    "\n",
    "            # Sample random noise vectors as generator input\n",
    "            noise = torch.randn(batch_size, INPUT_DIMENSION, 1, 1, device = DEVICE)\n",
    "\n",
    "            # Generate a batch of fake images from the noise\n",
    "            fake = dcgan_generator(noise)\n",
    "\n",
    "            # Save each generated image to the output directory\n",
    "            for j in range(batch_size):\n",
    "                save_image(\n",
    "                    fake[j], # Single image tensor\n",
    "                    os.path.join(class_output_directory, f\"gen_{i + j}.png\"),\n",
    "                    normalize = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bc362",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_generator_cordana' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      7\u001b[39m real_image_counts = {\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcordana\u001b[39m\u001b[33m\"\u001b[39m        : \u001b[32m145\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhealthy\u001b[39m\u001b[33m\"\u001b[39m        : \u001b[32m115\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpestalotiopsis\u001b[39m\u001b[33m\"\u001b[39m : \u001b[32m155\u001b[39m,\n\u001b[32m     11\u001b[39m }\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Dictionary mapping class labels to their corresponding trained generators\u001b[39;00m\n\u001b[32m     14\u001b[39m trained_generators = {\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcordana\u001b[39m\u001b[33m\"\u001b[39m        : \u001b[43mtrained_generator_cordana\u001b[49m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhealthy\u001b[39m\u001b[33m\"\u001b[39m        : trained_generator_healthy,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpestalotiopsis\u001b[39m\u001b[33m\"\u001b[39m : trained_generator_pestalotiopsis,\n\u001b[32m     18\u001b[39m }\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Generate synthetic images for each underrepresented class\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label, real_count \u001b[38;5;129;01min\u001b[39;00m real_image_counts.items():\n",
      "\u001b[31mNameError\u001b[39m: name 'trained_generator_cordana' is not defined"
     ]
    }
   ],
   "source": [
    "GAN_OUTPUT_DIRECTORY_BALANCED = \"../model2/balanced\"\n",
    "\n",
    "# Target count based on the dominant Sigatoka class\n",
    "TARGET_COUNT = 424\n",
    "\n",
    "# Dictionary of class names and their real image counts\n",
    "real_image_counts = {\n",
    "    \"cordana\"        : 145,\n",
    "    \"healthy\"        : 115,\n",
    "    \"pestalotiopsis\" : 155,\n",
    "}\n",
    "\n",
    "# Dictionary mapping class labels to their corresponding trained generators\n",
    "trained_generators = {\n",
    "    \"cordana\"        : trained_generator_cordana,\n",
    "    \"healthy\"        : trained_generator_healthy,\n",
    "    \"pestalotiopsis\" : trained_generator_pestalotiopsis,\n",
    "}\n",
    "\n",
    "# Generate synthetic images for each underrepresented class\n",
    "for label, real_count in real_image_counts.items():\n",
    "    amount_to_generate = TARGET_COUNT - real_count\n",
    "\n",
    "    generator = trained_generators[label]\n",
    "\n",
    "    generate_synthetic_images(\n",
    "        dcgan_generator = generator, amount_to_generate = amount_to_generate, class_label = label, output_directory = GAN_OUTPUT_DIRECTORY_BALANCED\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffba9e8",
   "metadata": {},
   "source": [
    "## Part 5: Training a DCGAN for feature extraction\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe371d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train DCGAN on balanced training data\n",
    "\n",
    "# balanced path is path to balanced training data\n",
    "# save path of model is automatic to \"../model2/final_dcgan\"\n",
    "\n",
    "trained_generator_final, trained_discriminator_final = train_dcgan(balanced_path = GAN_OUTPUT_DIRECTORY_BALANCED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
