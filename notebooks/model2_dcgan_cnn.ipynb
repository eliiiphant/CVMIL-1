{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88485067",
   "metadata": {},
   "source": [
    "## Importing Needed Libraries\n",
    "\n",
    "<p align=\"justify\"> This section loads essential libraries for building and training the DCGAN and CNN pipelines. It includes PyTorch modules for deep learning, torchvision for image utilities, and other auxiliary tools.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Data Preprocessing\n",
    "\n",
    "import torch\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset, random_split\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Part 2: Creating the DCGAN Generator and Discriminator Classes\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Part 3: Training a DCGAN for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from numpy import cos, pi\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d2cb2",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "<p align=\"justify\">This step prepares image data in formats suitable for GAN generation and CNN classification, with augmentation strategies tailored to each model's needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20f4ec",
   "metadata": {},
   "source": [
    "<p align=\"justify\">This code block defines key constants used throughout the pipeline: file paths, image sizes for GAN and CNN processing, and the target banana leaf classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "411d5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "RAW_DATA_DIR = \"../training_data\"\n",
    "GAN_SIZE = (128, 128)\n",
    "CNN_SIZE = (224, 224)\n",
    "BANANA_CLASSES  = [\"cordana\", \"healthy\", \"pestalotiopsis\", \"sigatoka\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89358d3e",
   "metadata": {},
   "source": [
    "<p align=\"justify\">The <code>set_seed</code> function enforces reproducible results by fixing random seeds across Python, NumPy, and PyTorch, and ensuring deterministic behavior on GPU.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dfc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set the seed for NumPy's random number generator\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set the seed for PyTorch's CPU RNG\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Set the seed for all CUDA devices (if using GPU)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure reproducibility by forcing deterministic behavior in cuDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # Disable benchmark mode to avoid non-deterministic algo selection\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234d1bd",
   "metadata": {},
   "source": [
    "<p align=\"justify\">Each model in the pipeline expects inputs in a specific format, so dedicated transform pipelines are defined:\n",
    "\n",
    "* <p align=\"justify\"><code>transform_gan_b</code> resizes images and normalizes pixel values to [-1, 1], as required by DCGANs, and is used for training the base GAN.\n",
    "* <p align=\"justify\"><code>transform_gan_p</code> adds stronger augmentations, including flips, color jitter, and affine transforms, to improve diversity in class-specific DCGAN training.\n",
    "* <p align=\"justify\"><code>transform_cnn</code> resizes images to 224×224 and converts them to tensors, preparing them for feature extraction and classification with the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_gan_b = transforms.Compose([\n",
    "    transforms.Resize(GAN_SIZE), # Resize for DCGAN\n",
    "    transforms.ToTensor(),       # To tensor\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5], \n",
    "        [0.5, 0.5, 0.5],\n",
    "    )  # Normalize to [-1, 1] for DCGAN\n",
    "])\n",
    "\n",
    "transform_gan_p = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(GAN_SIZE, scale = (0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(\n",
    "            brightness = 0.2, \n",
    "            contrast   = 0.2, \n",
    "            saturation = 0.2, \n",
    "            hue        = 0.05,\n",
    "        ),\n",
    "    ], p = 0.7),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAffine(\n",
    "            degrees   = 10, \n",
    "            translate = (0.1, 0.1), \n",
    "            scale     = (0.9, 1.0), \n",
    "        ),\n",
    "    ], p = 0.7),\n",
    "])\n",
    "\n",
    "transform_cnn = transforms.Compose([\n",
    "    transforms.Resize(CNN_SIZE), # Resize for CNN\n",
    "    transforms.ToTensor(),       # To tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee0ae5",
   "metadata": {},
   "source": [
    "<p align=\"justify\">These helper functions prepare image batches tailored to the training objectives:\n",
    "\n",
    "* <p align=\"justify\"><code>load_gan_data(...)</code> loads images for DCGAN training. If a <code>target_class</code> is specified, it filters that class and applies heavy augmentations to generate multiple variants per image, which helps address class imbalance during synthetic generation.\n",
    "* <p align=\"justify\"><code>load_cnn_data(...)</code> prepares the dataset for CNN classification using simpler transforms that standardize size and format without augmentations.\n",
    "\n",
    "<p align=\"justify\">Both return PyTorch DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e02535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gan_data(batch_size = 32, workers = 4, target_class = None, num_variants = 10, seed = 42, directory = RAW_DATA_DIR):\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    # Load full dataset with base GAN transformations\n",
    "    dataset_gan = datasets.ImageFolder(root = directory, transform = transform_gan_b)\n",
    "\n",
    "    if target_class:\n",
    "        # Get class index from the class name\n",
    "        class_index = dataset_gan.class_to_idx[target_class]\n",
    "\n",
    "        # Filter indices where target matches\n",
    "        indices = [i for i, (_, label) in enumerate(dataset_gan.samples) if label == class_index]\n",
    "\n",
    "        # Wrap in a Subset\n",
    "        dataset_gan = Subset(dataset_gan, indices)\n",
    "\n",
    "        # Create a list to store augmented images\n",
    "        augmented_images = []\n",
    "\n",
    "        rng = random.Random(seed)\n",
    "\n",
    "        # Apply augmentations to each image in the loaded dataset\n",
    "        for i in range(len(dataset_gan)):\n",
    "            image, label = dataset_gan[i]\n",
    "\n",
    "            # Generate num_variants augmented versions of image\n",
    "            for _ in range(num_variants):\n",
    "                torch.manual_seed(rng.randint(0, 999999))\n",
    "        \n",
    "                augmented_image = transform_gan_p(image)\n",
    "\n",
    "                augmented_images.append((augmented_image, label))\n",
    "\n",
    "        # Create new dataset with augmented images\n",
    "        final_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.stack([image for image, _ in augmented_images]),  # Stack all augmented images\n",
    "            torch.tensor([label for _, label in augmented_images])  # Stack all labels\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        final_dataset = dataset_gan\n",
    "\n",
    "    # Create DataLoader for the GAN data\n",
    "    dataloader_gan = DataLoader(final_dataset, batch_size = batch_size, shuffle = True, num_workers = workers, generator = generator)\n",
    "\n",
    "    return dataloader_gan\n",
    "\n",
    "def load_cnn_data(batch_size = 32, workers = 4):\n",
    "    # Load dataset with CNN transformations\n",
    "    dataset_cnn = datasets.ImageFolder(root=RAW_DATA_DIR, transform = transform_cnn)\n",
    "    \n",
    "    # Create DataLoader for the CNN data\n",
    "    dataloader_cnn = DataLoader(dataset_cnn, batch_size=batch_size, shuffle = True, num_workers = workers)\n",
    "\n",
    "    return dataloader_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2af1f2",
   "metadata": {},
   "source": [
    "## Part 2: Creating the DCGAN Generator and Discriminator Classes\n",
    "\n",
    "<p align=\"justify\">This part implements the core components of the DCGAN architecture. The Generator learns to produce banana leaf images from random noise, while the Discriminator distinguishes between real and synthetic samples, forming the adversarial training loop.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc649e",
   "metadata": {},
   "source": [
    "<p align=\"justify\">The code block below defines all hyperparameters and training settings for consistent execution across GAN and CNN components. It also sets device preferences and ensures reproducibility through a fixed seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "SEED_NUM = 42              # Seed number for reproducibility\n",
    "\n",
    "BATCH_SIZE = 128           # Number of images per training batch\n",
    "\n",
    "INPUT_DIMENSION = 100      # Dimensionality of the generator input\n",
    "\n",
    "NC = 3                     # Number of channels in the training images\n",
    "\n",
    "NGF = 64                   # Base number of feature maps in the Generator\n",
    "\n",
    "NDF = 64                   # Base number of feature maps in the Discriminator\n",
    "\n",
    "EPOCHS = 200               # Number of training epochs\n",
    "\n",
    "CHECKPOINT = 10            # Checkpoint number for model saving\n",
    "\n",
    "LEARNING_RATE_G = 0.0002   # Learning rate for the Generator\n",
    "\n",
    "LEARNING_RATE_D = 0.0001   # Learning rate for the Discriminator\n",
    "\n",
    "LEARNING_RATE_CNN = 0.0001 # Learning rate for the CNN\n",
    "\n",
    "BETA1 = 0.5                # Beta1 value for the Adam optimizer to help stabilize DCGAN training\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "NGPU = 1  # Number of GPUs to use (0 means CPU only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fe897",
   "metadata": {},
   "source": [
    "<p align=\"justify\">The Generator class defines a transposed convolutional neural network that gradually upsamples a 100-dimensional latent vector into a 128×128 RGB image. It uses a series of ConvTranspose2d layers paired with InstanceNorm and ReLU activations, capped with a Tanh function to output pixel values scaled to [-1, 1]. Dropout is included at each block to promote regularization and prevent overfitting, which is useful in settings with limited real training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d25128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Generator network composed of a stack of transposed conv blocks\n",
    "        self.main = nn.Sequential(\n",
    "            self._block(INPUT_DIMENSION, NGF * 16, 4, 1, 0, bias = False),  # First layer: latent vector -> feature map\n",
    "            self._block(NGF * 16, NGF * 8, 4, 2, 1, bias = False),          # Upsample to 8 x 8\n",
    "            self._block(NGF * 8, NGF * 4, 4, 2, 1, bias = False),           # Upsample to 16 x 16\n",
    "            self._block(NGF * 4, NGF * 2, 4, 2, 1, bias = False),           # Upsample to 32 x 32\n",
    "            self._block(NGF * 2, NGF, 4, 2, 1, bias = False),               # Upsample to 64 x 64\n",
    "\n",
    "            nn.ConvTranspose2d(NGF, NC, 4, 2, 1, bias = False),             # Final upsample to 128 x 128 with RGB output\n",
    "            nn.Tanh()                                                       # Output pixel values in [-1, 1]\n",
    "        )\n",
    "\n",
    "    # Helper function to define a generator block:\n",
    "\n",
    "    # ConvTranspose2d -> InstanceNorm2d -> ReLU -> Dropout\n",
    "\n",
    "    def _block(self, i_channels, o_channels, kernel_size, stride, padding, bias):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                i_channels, \n",
    "                o_channels, \n",
    "                kernel_size, \n",
    "                stride, \n",
    "                padding, \n",
    "                bias = bias),\n",
    "            nn.InstanceNorm2d(o_channels, affine = True),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3) # Dropout to help regularize on small data\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78349a5e",
   "metadata": {},
   "source": [
    "<p align=\"justify\">On the other hand, the Discriminator is a deep convolutional network designed to classify images as real or fake by progressively downsampling them. Each block applies standard convolution, optional InstanceNorm, and LeakyReLU, with dropout for robustness. Notably, the first layer skips normalization to preserve raw signal. A helper flag in <code>forward()</code> optionally returns intermediate features for CNN training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7581479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Discriminator network composed of downsampling conv blocks\n",
    "        self.main = nn.Sequential(\n",
    "            self._block(NC, NDF, 4, 2, 1, bias = False, use_instanceNorm2d = False), # First block: no InstanceNorm2d\n",
    "            self._block(NDF, NDF *  2, 4, 2, 1, bias = False),                       # Downsample to 32 x 32\n",
    "            self._block(NDF * 2, NDF *  4, 4, 2, 1, bias = False),                   # Downsample to 16 x 16\n",
    "            self._block(NDF * 4, NDF *  8, 4, 2, 1, bias = False),                   # Downsample to 8 x 8\n",
    "            self._block(NDF * 8, NDF * 16, 4, 2, 1, bias = False),                   # Downsample to 4 x 4\n",
    "\n",
    "            nn.Conv2d(NDF * 16, 1, 4, 1, 0, bias = False),                           # Final layer: reduce to 1 x 1\n",
    "        )\n",
    "\n",
    "    # Helper function to define a discriminator block:\n",
    "\n",
    "    # Conv2d -> (optional) InstanceNorm2d -> LeakyReLU\n",
    "\n",
    "    def _block(self, i_channels, o_channels, kernel_size, stride, padding, bias, use_instanceNorm2d = True):\n",
    "        layers = [nn.Conv2d(\n",
    "            i_channels, \n",
    "            o_channels, \n",
    "            kernel_size, \n",
    "            stride, \n",
    "            padding, \n",
    "            bias = bias)]\n",
    "        \n",
    "        if use_instanceNorm2d:\n",
    "            layers.append(nn.InstanceNorm2d(o_channels, affine = True))\n",
    "        \n",
    "        layers.append(nn.LeakyReLU(0.2, inplace = True))\n",
    "        layers.append(nn.Dropout(0.3)) # Dropout to help regularize on small data\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inp, return_features=False):\n",
    "        x = inp\n",
    "\n",
    "        for i, layer in enumerate(self.main):\n",
    "            x = layer(x)\n",
    "            if return_features and i == 4: \n",
    "                return x \n",
    "\n",
    "        return x            \n",
    "        # return self.main(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb85cf",
   "metadata": {},
   "source": [
    "## Part 3: Training a DCGAN for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "<p align=\"justify\"> In this stage, we train a dedicated DCGAN for each underrepresented class: Cordana, Healthy, and Pestalotiopsis. The training loop involves alternating between updating the Discriminator to better distinguish real from fake images and guiding the Generator to produce more realistic outputs. Over multiple epochs, this adversarial process helps each model learn the visual distribution of its respective class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b3a95",
   "metadata": {},
   "source": [
    "<p align=\"justify\">The code block below is sourced from the official PyTorch documentation and follows the DCGAN paper’s recommended weight initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "014a3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find(\"InstanceNorm\") != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99b790",
   "metadata": {},
   "source": [
    "<p align=\"justify\">This function sets up the output directory structure for saving GAN-generated images during training. It ensures that each class except Sigatoka has a clean folder unless training is resumed, in which case existing outputs are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20fa7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_OUTPUT_DIRECTORY_TEST = \"../model2/gan_test\" # for debugging while training\n",
    "\n",
    "def prepare_output_directory(resume = False):\n",
    "    for cls in [\"base\"] + BANANA_CLASSES:\n",
    "        if cls != \"sigatoka\":\n",
    "            full_path = Path(GAN_OUTPUT_DIRECTORY_TEST) / cls\n",
    "\n",
    "            # Only remove and recreate the directory if not resuming\n",
    "            if not resume and full_path.exists():\n",
    "                shutil.rmtree(full_path)\n",
    "\n",
    "            full_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# prepare_output_directory(resume = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c89be",
   "metadata": {},
   "source": [
    "<p align=\"justify\">This function is the core training loop for a DCGAN, designed to generate synthetic banana leaf images. It is flexible enough to handle training under three scenarios: (1) training a base DCGAN from scratch or a checkpoint, (2) initializing a class-specific DCGAN from the base model, and (3) resuming training from a class-specific checkpoint.\n",
    "\n",
    "<p align=\"justify\"> Some notes on the training loop setup:\n",
    "\n",
    "* <p align=\"justify\"><code>BCEWithLogitsLoss</code> is utilized for Generator and Discriminator losses.\n",
    "* <p align=\"justify\">The labels for real and fake images are smoothed slightly at 0.9 and 0.1, respectively, instead of 1 and 0, to regularize training and reduce overconfidence in the Discriminator.\n",
    "* <p align=\"justify\">Cosine annealing learning rate schedulers are used to gradually reduce the learning rate over time and help the model converge more smoothly.\n",
    "\n",
    "\n",
    "<p align=\"justify\">The training loop works as such:\n",
    "\n",
    "### Discriminator Update\n",
    "\n",
    "1. <p align=\"justify\">Real images are passed through the Discriminator after adding decaying Gaussian noise, a form of instance noise that regularizes training and helps prevent overfitting.\n",
    "2. <p align=\"justify\">Fake images are generated using the Generator and also passed through the Discriminator.\n",
    "3. <p align=\"justify\">The Discriminator is trained to correctly distinguish between real and fake using the sum of the two BCE losses.\n",
    "4. <p align=\"justify\">Gradients for the Discriminator are clipped to stabilize learning.\n",
    "\n",
    "### Generator Update\n",
    "\n",
    "<p align=\"justify\">The Generator is then updated with the goal of “fooling” the Discriminator. It backpropagates the error from the Discriminator’s output using the real label. (trying to make fake images look real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff876c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dcgan(target_class = None, resume = False, checkpoint_path = None, balanced_path = None):\n",
    "    # Set random seed for reproducibility\n",
    "    set_seed(SEED_NUM)\n",
    "\n",
    "    # Load dataset and define save path\n",
    "    if balanced_path:  \n",
    "        save_path = \"../model2/final_dcgan\"\n",
    "\n",
    "        dataloader = load_gan_data(batch_size = BATCH_SIZE, directory = balanced_path)\n",
    "\n",
    "    else:\n",
    "        save_path = GAN_OUTPUT_DIRECTORY_TEST\n",
    "\n",
    "        if target_class:\n",
    "            dataloader = load_gan_data(batch_size = BATCH_SIZE, target_class = target_class)\n",
    "\n",
    "        else:\n",
    "            dataloader = load_gan_data(batch_size = BATCH_SIZE)\n",
    "\n",
    "    # Initialize Generator and Discriminator\n",
    "    netG = Generator(ngpu = NGPU).to(DEVICE)\n",
    "    netD = Discriminator(ngpu = NGPU).to(DEVICE)\n",
    "\n",
    "    # Handle multi-GPU setup if applicable\n",
    "    if (DEVICE.type == \"cuda\") and (NGPU > 1):\n",
    "        netG = nn.DataParallel(netG, list(range(NGPU)))\n",
    "        netD = nn.DataParallel(netD, list(range(NGPU)))\n",
    "\n",
    "    # Fixed noise for generating sample outputs and tracking progress during training\n",
    "    fixed_generator = torch.Generator(device=DEVICE).manual_seed(SEED_NUM)\n",
    "\n",
    "    fixed_noise = torch.randn(64, INPUT_DIMENSION, 1, 1, device = DEVICE, generator = fixed_generator)\n",
    "\n",
    "    # Optimizers for Generator and Discriminator\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr = LEARNING_RATE_D, betas = (BETA1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr = LEARNING_RATE_G, betas = (BETA1, 0.999))\n",
    "\n",
    "    # Schedulers for optimizers\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerD, T_max = EPOCHS, eta_min = 1e-6)\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerG, T_max = EPOCHS, eta_min = 1e-6)\n",
    "\n",
    "    # Default starting epoch\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Loads checkpoint dcgan if provided \n",
    "\n",
    "    # Case 1: train base dcgan from a checkpoint\n",
    "    # Case 2: train per-class dcgan from scratch with base dcgan weights as a starting point\n",
    "    # Case 3: train per-class dcgan from a checkpoint\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location = DEVICE)\n",
    "        netG.load_state_dict(checkpoint[\"netG\"])\n",
    "        netD.load_state_dict(checkpoint[\"netD\"])\n",
    "\n",
    "        # Loads checkpoint optimizers if resuming training\n",
    "        if resume:\n",
    "            optimizerG.load_state_dict(checkpoint[\"optimizerG\"])\n",
    "            optimizerD.load_state_dict(checkpoint[\"optimizerD\"])\n",
    "            start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    # Fresh start\n",
    "    else:\n",
    "        netG.apply(initialize_weights)\n",
    "        netD.apply(initialize_weights)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Labels for real and fake images\n",
    "    real_label = 0.9 # Slightly less than 1\n",
    "    fake_label = 0.1 # Slightly more than 0\n",
    "\n",
    "    # Actual training\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        # Calculate noise magnitude decay\n",
    "        noise_magnitude = 0.1 * 0.5 * (1 + cos(pi * epoch / EPOCHS)) # 0.1 = maximum noise magnitude\n",
    "    \n",
    "        for i, (real_images, _) in enumerate(dataloader): # Iterate through batches in the dataset\n",
    "            # 1. Update Discriminator: \n",
    "            #    maximize log(D(x)) + log(1 - D(G(z)))\n",
    "\n",
    "            # 1.A. Train Discriminator on real images\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # Format real batch\n",
    "            real_images = real_images.to(DEVICE)\n",
    "\n",
    "            # Well, train the Discriminator on noisy real images\n",
    "            noise = torch.randn_like(real_images) * noise_magnitude # noise magnitude decays\n",
    "\n",
    "            noisy_real_images = real_images + noise\n",
    "\n",
    "            size = real_images.size(0)\n",
    "\n",
    "            label = torch.full((size,), real_label, dtype = torch.float, device = DEVICE)\n",
    "\n",
    "            # Forward pass noisy real images through Discriminator\n",
    "            output = netD(noisy_real_images).view(-1)\n",
    "\n",
    "            # Calculate Discriminator loss for noisy real images\n",
    "            errD_real = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for noisy real images\n",
    "            errD_real.backward()\n",
    "\n",
    "            # Mean output for noisy real images\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # 1.B. Train Discriminator on batch of all fake images\n",
    "\n",
    "            # Generate batch of latent vectors\n",
    "            generator = torch.Generator(device=DEVICE).manual_seed(SEED_NUM + epoch)\n",
    "\n",
    "            noise = torch.randn(size, INPUT_DIMENSION, 1, 1, device = DEVICE, generator = generator)\n",
    "\n",
    "            # Generate fake images with Generator\n",
    "            fake = netG(noise)\n",
    "\n",
    "            # Classify fake images with Discriminator\n",
    "            label.fill_(fake_label)\n",
    "\n",
    "            # Forward pass fake images through Discriminator\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "\n",
    "            # Calculate Discriminator loss for fake images\n",
    "            errD_fake = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for fake images\n",
    "            errD_fake.backward()\n",
    "\n",
    "            # Clip Discriminator gradients for stability\n",
    "            torch.nn.utils.clip_grad_norm_(netD.parameters(), max_norm = 1.0)\n",
    "\n",
    "            # Mean output for fake images\n",
    "            D_G_z1 = output.mean().item()\n",
    "\n",
    "            # Compute total Discriminator error = real error + fake error\n",
    "            errD = errD_real + errD_fake\n",
    "\n",
    "            # Finally update Discriminator\n",
    "            optimizerD.step()\n",
    "\n",
    "            # 2. Update Generator: \n",
    "            #    maximize log(D(G(z)))\n",
    "\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for Generator cost\n",
    "\n",
    "            # Pass fake images through Discriminator\n",
    "            output = netD(fake).view(-1)\n",
    "\n",
    "            # Calculate Generator loss based on Discriminator's output\n",
    "            errG = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for Generator\n",
    "            errG.backward()\n",
    "\n",
    "            # Mean output for fake images after Generator update\n",
    "            D_G_z2 = output.mean().item()\n",
    "\n",
    "            # Finally update Generator\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Debugging: Print losses and monitor training progress\n",
    "\n",
    "            if i in [0, len(dataloader)//2]:\n",
    "                print(\n",
    "                  f\"Epoch [{epoch}/{EPOCHS}] Batch {i}/{len(dataloader)} \\\n",
    "                    Loss D: {errD.item():.4f}, loss G: {errG.item():.4f} \\\n",
    "                    D(x): {D_x:.4f}, \\\n",
    "                    D(G(z))_real: {D_G_z1:.4f}, D(G(z))_fake: {D_G_z2:.4f}\"\n",
    "                )\n",
    "\n",
    "        # Step learning rate schedulers\n",
    "        schedulerD.step()\n",
    "        schedulerG.step()\n",
    "\n",
    "        if epoch % CHECKPOINT == 0:\n",
    "            fake_images = netG(fixed_noise).detach()\n",
    "\n",
    "            if target_class:\n",
    "                path = f\"{save_path}/{target_class}\"\n",
    "            \n",
    "            else:\n",
    "                # root folder of save path, else /base if base dcgan\n",
    "                path = f\"{save_path}{\"\" if balanced_path else \"/base\"}\"\n",
    "\n",
    "            # Save images Generator could produce during checkpoints\n",
    "            save_image(\n",
    "                fake_images,\n",
    "                os.path.join(path, f\"sample_epoch_{epoch}.png\"),\n",
    "                normalize = True\n",
    "            )\n",
    "\n",
    "            # Save model version\n",
    "            save_dict = {\n",
    "                \"epoch\": epoch,\n",
    "                \"netG\": netG.state_dict(),\n",
    "                \"netD\": netD.state_dict(),\n",
    "                \"optimizerG\": optimizerG.state_dict(),\n",
    "                \"optimizerD\": optimizerD.state_dict(),\n",
    "            }\n",
    "\n",
    "            torch.save(save_dict, os.path.join(path, f\"checkpoint_epoch_{epoch}.pth\"))\n",
    "\n",
    "    torch.save({\n",
    "        \"epoch\": EPOCHS,\n",
    "        \"netG\": netG.state_dict(),\n",
    "        \"netD\": netD.state_dict(),\n",
    "        \"optimizerG\": optimizerG.state_dict(),\n",
    "        \"optimizerD\": optimizerD.state_dict(),\n",
    "    }, os.path.join(f\"{save_path}\", f\"{target_class if target_class else (\"final\" if balanced_path else \"base\")}_dcgan_final.pth\"))\n",
    "\n",
    "    return netG, netD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be787ca7",
   "metadata": {},
   "source": [
    "### Part 3.1: Training a Base DCGAN\n",
    "\n",
    "<p align=\"justify\">We first train a base DCGAN on the full original dataset. This model captures general features across all classes and serves as a weight initialization checkpoint when training class-specific DCGANs to help improve convergence and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Base DCGAN\n",
    "\n",
    "# trained_generator_base, trained_discriminator_base = train_dcgan(target_class = None, resume = False, checkpoint_path = None)\n",
    "\n",
    "trained_generator_base, trained_discriminator_base = train_dcgan(target_class = None, resume = True, checkpoint_path = '../model2/gan_test/base_dcgan_final.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6e100",
   "metadata": {},
   "source": [
    "### Part 3.2: Training a DCGAN Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09fd2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DCGAN for \"cordana\" class\n",
    "# trained_generator_cordana, _ = train_dcgan(target_class = \"cordana\")\n",
    "\n",
    "trained_generator_cordana, _ = train_dcgan(target_class = \"cordana\", resume = True, checkpoint_path = \"../model2/gan_test/cordana_dcgan_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe98212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DCGAN for \"healthy\" class\n",
    "# trained_generator_healthy, _ = train_dcgan(target_class = \"healthy\")\n",
    "\n",
    "trained_generator_healthy, _ = train_dcgan(target_class = \"healthy\", resume = True, checkpoint_path = '../model2/gan_test/healthy_dcgan_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e819ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DCGAN for \"pestalotiopsis\" class\n",
    "# trained_generator_pestalotiopsis, _ = train_dcgan(target_class = \"pestalotiopsis\")\n",
    "\n",
    "trained_generator_pestalotiopsis, _ = train_dcgan(target_class = \"pestalotiopsis\", resume = True, checkpoint_path = \"../model2/gan_test/pestalotiopsis_dcgan_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3e960",
   "metadata": {},
   "source": [
    "## Part 4: Generating Images for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "<p align=\"justify\">To tackle class imbalance, separate DCGANs were previously trained for each underrepresented class: Cordana, Healthy, and Pestalotiopsis. These DCGANs generate synthetic images that augment the original dataset, equalizing the number of samples and hopefully helping improve classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b202ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_images(dcgan_generator, amount_to_generate, class_label, output_directory):\n",
    "\n",
    "    # Set the generator to evaluation mode to disable Dropout and InstanceNorm2d updates\n",
    "    dcgan_generator.eval()\n",
    "\n",
    "    # Construct the path to the class-specific output directory\n",
    "    class_output_directory = os.path.join(output_directory, class_label)\n",
    "\n",
    "    # Create the output directory if it does not exist just in case\n",
    "    os.makedirs(class_output_directory, exist_ok = True)\n",
    "\n",
    "    # Disable gradient computation for efficiency during inference\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, amount_to_generate, 16): # Batches of 16\n",
    "            batch_size = min(16, amount_to_generate - i) # Adjusts batch size if near the end of generation\n",
    "\n",
    "            # Sample random noise vectors as generator input\n",
    "            noise = torch.randn(batch_size, INPUT_DIMENSION, 1, 1, device = DEVICE)\n",
    "\n",
    "            # Generate a batch of fake images from the noise\n",
    "            fake = dcgan_generator(noise)\n",
    "\n",
    "            # Save each generated image to the output directory\n",
    "            for j in range(batch_size):\n",
    "                save_image(\n",
    "                    fake[j], # Single image tensor\n",
    "                    os.path.join(class_output_directory, f\"gen_{i + j}.png\"),\n",
    "                    normalize = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d5bc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_OUTPUT_DIRECTORY_BALANCED = \"../model2/balanced\"\n",
    "\n",
    "# Target count based on the dominant Sigatoka class\n",
    "TARGET_COUNT = 424\n",
    "\n",
    "# Dictionary of class names and their real image counts\n",
    "real_image_counts = {\n",
    "    \"cordana\"        : 145,\n",
    "    \"healthy\"        : 115,\n",
    "    \"pestalotiopsis\" : 155,\n",
    "}\n",
    "\n",
    "# Dictionary mapping class labels to their corresponding trained generators\n",
    "trained_generators = {\n",
    "    \"cordana\"        : trained_generator_cordana,\n",
    "    \"healthy\"        : trained_generator_healthy,\n",
    "    \"pestalotiopsis\" : trained_generator_pestalotiopsis,\n",
    "}\n",
    "\n",
    "# Generate synthetic images for each underrepresented class\n",
    "for label, real_count in real_image_counts.items():\n",
    "    amount_to_generate = TARGET_COUNT - real_count\n",
    "\n",
    "    generator = trained_generators[label]\n",
    "\n",
    "    generate_synthetic_images(\n",
    "        dcgan_generator = generator, amount_to_generate = amount_to_generate, class_label = label, output_directory = GAN_OUTPUT_DIRECTORY_BALANCED\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffba9e8",
   "metadata": {},
   "source": [
    "## Part 5: Training a Unified DCGAN for Feature Extraction\n",
    "\n",
    "<p align=\"justify\">A single DCGAN was trained on the balanced dataset combining real and synthetic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fe371d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train DCGAN on balanced training data\n",
    "\n",
    "# balanced path is path to balanced training data\n",
    "# save path of model is automatic to \"../model2/final_dcgan\"\n",
    "\n",
    "_, trained_discriminator_final = train_dcgan(resume = True, checkpoint_path = \"../model2/final_dcgan/final_dcgan_final.pth\", balanced_path = GAN_OUTPUT_DIRECTORY_BALANCED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088dc2fd",
   "metadata": {},
   "source": [
    "## Part 6: Extract features from final DCGAN\n",
    "\n",
    "<p align=\"justify\">After training, the discriminator is frozen and repurposed as a fixed feature extractor.\n",
    "\n",
    "<p align=\"justify\">Mid-level feature maps were extracted from the frozen discriminator’s intermediate layers for each image. These feature representations serve as inputs to the CNN classifier used to enhance its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac4a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(discriminator = None, dataset = None):\n",
    "    # Set the discriminator model to evaluation mode to disable dropout etc. etc.\n",
    "    discriminator.eval()\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    set_seed(SEED_NUM)\n",
    "\n",
    "    # Create a DataLoader from the given dataset\n",
    "    dataloader = DataLoader(dataset, batch_size = 32, shuffle = False, num_workers=4)\n",
    "\n",
    "    # Lists to accumulate extracted features and labels\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    discriminator.eval() #\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            # Move images to the same device as the model\n",
    "            images = images.to(DEVICE)\n",
    "\n",
    "            # Forward pass through discriminator to get intermediate features\n",
    "            features = discriminator(images, return_features = True) \n",
    "\n",
    "            # Move features to CPU and accumulate\n",
    "            all_features.append(features.cpu())\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Concatenate all feature batches into a single tensor of shape (total_images, N)\n",
    "    all_features = torch.cat(all_features) \n",
    "\n",
    "    # Concatenate all label batches into a single tensor of shape (total_images,)\n",
    "    all_labels = torch.cat(all_labels) \n",
    "\n",
    "    return all_features, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e216d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_imagefolder_dataset(root_path, train_ratio = 0.8):\n",
    "    # Load all images from the directory and apply transform\n",
    "    full_dataset = datasets.ImageFolder(root = root_path, transform = transform_cnn)\n",
    "\n",
    "    # Compute train/test split sizes\n",
    "    train_size = int(train_ratio * len(full_dataset))\n",
    "\n",
    "    test_size = len(full_dataset) - train_size\n",
    "\n",
    "    # Seed for reproducibility\n",
    "    generator = torch.Generator().manual_seed(SEED_NUM)\n",
    "\n",
    "    # Randomly split dataset into train and test subsets\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size], generator = generator)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = split_imagefolder_dataset(\"../model2/balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c75b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(discriminator = trained_discriminator_final, dataset = train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c127158",
   "metadata": {},
   "source": [
    "## Part 7: Define CNN architecture and Train Final CNN Classifier\n",
    "\n",
    "<p align=\"justify\">With features extracted from the frozen discriminator, a CNN is designed to perform the final classification of banana leaf diseases. The network is trained on these extracted features to optimize classification accuracy, completing the hybrid model pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a042b497",
   "metadata": {},
   "source": [
    "<p align=\"justify\">This CNN takes feature maps extracted from the frozen discriminator as input and applies three convolutional layers with batch normalization and ReLU activations to learn higher-level representations. It then uses adaptive average pooling to reduce spatial dimensions, followed by dropout and a fully connected layer to output class scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c91d96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, discriminator = None, input_channels=1024, num_classes=len(BANANA_CLASSES)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.discriminator = discriminator\n",
    "        # self.discriminator.eval() \n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1, 1)) \n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),  \n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413bce9",
   "metadata": {},
   "source": [
    "<p align=\"justify\"><code>cnn_classifier</code> trains the CNN using extracted features and labels wrapped in a DataLoader for batch processing. It supports loading from a checkpoint to resume training and saves intermediate checkpoints periodically to enable recovery. The model is optimized using cross-entropy loss and Adam optimizer over multiple epochs, with progress printed after each epoch, and the final trained model is saved before returning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81610cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_classifier(batch_size = 32, workers = 4, epochs = 25, features = None, labels = None, checkpoint_path = None, discriminator = None):\n",
    "    # Combine features and labels into a PyTorch dataset\n",
    "    feature_dataset = TensorDataset(features, labels)\n",
    "\n",
    "    feature_loader = DataLoader(feature_dataset, batch_size = batch_size, shuffle = True, num_workers = workers)\n",
    "\n",
    "    # Initialize CNN classifier\n",
    "    model = CNNClassifier(discriminator).to(DEVICE)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE_CNN)\n",
    "\n",
    "    # Default starting epoch\n",
    "    start_epoch = 0\n",
    "\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location = DEVICE)\n",
    "\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    # TO ADD: setting up of save folder!\n",
    "        \n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        for batch_x, batch_y in feature_loader:\n",
    "            # Move input features and labels to the computing device\n",
    "            batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
    "\n",
    "            # Zero the gradients from the previous step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to compute predictions\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # Compute loss between predicted and actual labels\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Debugging: Print losses\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]: Loss = {loss.item():.4f}\")\n",
    "\n",
    "        if epoch % CHECKPOINT == 0:\n",
    "            checkpoint_path = os.path.join(\"../model2/cnn_classifier\", f\"cnn_classifier_epoch_{epoch}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        \n",
    "\n",
    "    checkpoint_path = os.path.join(\"../model2/cnn_classifier\", f\"cnn_classifier_final.pth\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"discriminator_state\": discriminator.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,                            \n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19bd8c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]: Loss = 0.4857\n",
      "Epoch [2/30]: Loss = 0.6707\n",
      "Epoch [3/30]: Loss = 0.2512\n",
      "Epoch [4/30]: Loss = 0.5004\n",
      "Epoch [5/30]: Loss = 0.2500\n",
      "Epoch [6/30]: Loss = 0.1754\n",
      "Epoch [7/30]: Loss = 0.2775\n",
      "Epoch [8/30]: Loss = 0.2244\n",
      "Epoch [9/30]: Loss = 0.1882\n",
      "Epoch [10/30]: Loss = 0.3450\n",
      "Epoch [11/30]: Loss = 0.1142\n",
      "Epoch [12/30]: Loss = 0.1611\n",
      "Epoch [13/30]: Loss = 0.1532\n",
      "Epoch [14/30]: Loss = 0.1197\n",
      "Epoch [15/30]: Loss = 0.0710\n",
      "Epoch [16/30]: Loss = 0.1028\n",
      "Epoch [17/30]: Loss = 0.1232\n",
      "Epoch [18/30]: Loss = 0.0974\n",
      "Epoch [19/30]: Loss = 0.0533\n",
      "Epoch [20/30]: Loss = 0.0548\n",
      "Epoch [21/30]: Loss = 0.0244\n",
      "Epoch [22/30]: Loss = 0.0820\n",
      "Epoch [23/30]: Loss = 0.0420\n",
      "Epoch [24/30]: Loss = 0.0627\n",
      "Epoch [25/30]: Loss = 0.0239\n",
      "Epoch [26/30]: Loss = 0.0381\n",
      "Epoch [27/30]: Loss = 0.1339\n",
      "Epoch [28/30]: Loss = 0.0154\n",
      "Epoch [29/30]: Loss = 0.0225\n",
      "Epoch [30/30]: Loss = 0.0968\n"
     ]
    }
   ],
   "source": [
    "cnn_model = cnn_classifier(epochs = 30, features = train_features, labels = train_labels, discriminator = trained_discriminator_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f27e0",
   "metadata": {},
   "source": [
    "# Part 8: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61f4c5",
   "metadata": {},
   "source": [
    "<p align=\"justify\">After training, the CNN classifier is evaluated on unseen test images to assess its ability to correctly identify different banana leaf diseases. Performance metrics such as accuracy, precision, recall, and F1-score are calculated to provide a comprehensive view of how well the model distinguishes between each banana leaf class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = extract_features(discriminator = trained_discriminator_final, dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 32,\n",
    "    shuffle = False,\n",
    "    num_workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, class_names, device = DEVICE):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(all_labels, all_preds, target_names = class_names))\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "evaluate_model(model = cnn_model, dataloader = test_loader, class_names = BANANA_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a043d8ec",
   "metadata": {},
   "source": [
    "# Model 2 References:\n",
    "\n",
    "1. https://docs.pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "\n",
    "2. https://pyimagesearch.com/2021/10/25/training-a-dcgan-in-pytorch/\n",
    "\n",
    "3. https://medium.com/@manoharmanok/implementing-dcgan-in-pytorch-using-the-celeba-dataset-a-comprehensive-guide-660e6e8e29d2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
