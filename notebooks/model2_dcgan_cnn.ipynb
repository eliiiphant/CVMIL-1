{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88485067",
   "metadata": {},
   "source": [
    "## Importing Needed Libraries\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Data Preprocessing\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Part 2: Creating the DCGAN Generator and Discriminator Classes\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Part 3: Training a DCGAN for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "\n",
    "# https://docs.pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "# https://pyimagesearch.com/2021/10/25/training-a-dcgan-in-pytorch/\n",
    "# https://medium.com/@manoharmanok/implementing-dcgan-in-pytorch-using-the-celeba-dataset-a-comprehensive-guide-660e6e8e29d2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d2cb2",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "411d5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "RAW_DATA_DIR = \"../training_data\"\n",
    "GAN_SIZE = (128, 128)\n",
    "CNN_SIZE = (224, 224)\n",
    "BANANA_CLASSES  = [\"cordana\", \"healthy\", \"pestalotiopsis\", \"sigatoka\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234d1bd",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_gan_b = transforms.Compose([\n",
    "    transforms.Resize(GAN_SIZE), # Resize for DCGAN\n",
    "    transforms.ToTensor(),       # To tensor\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5], \n",
    "        [0.5, 0.5, 0.5],\n",
    "    )  # Normalize to [-1, 1] for DCGAN\n",
    "])\n",
    "\n",
    "transform_gan_p = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(\n",
    "        brightness = 0.2, \n",
    "        contrast   = 0.2, \n",
    "        saturation = 0.2, \n",
    "        hue        = 0.2,\n",
    "    ),\n",
    "    transforms.RandomAffine(10),\n",
    "    transforms.RandomResizedCrop(GAN_SIZE, scale = (0.8, 1.0)),\n",
    "])\n",
    "\n",
    "transform_cnn = transforms.Compose([\n",
    "    transforms.Resize(CNN_SIZE), # Resize for CNN\n",
    "    transforms.ToTensor(),       # To tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee0ae5",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e02535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gan_data(batch_size = 32, workers = 4, target_class = None, num_variants = 10):\n",
    "    # Load full dataset with base GAN transformations\n",
    "    dataset_gan = datasets.ImageFolder(root = RAW_DATA_DIR, transform = transform_gan_b)\n",
    "\n",
    "    if target_class:\n",
    "        # Get class index from the class name\n",
    "        class_index = dataset_gan.class_to_idx[target_class]\n",
    "\n",
    "        # Filter indices where target matches\n",
    "        indices = [i for i, (_, label) in enumerate(dataset_gan.samples) if label == class_index]\n",
    "\n",
    "        # Wrap in a Subset\n",
    "        dataset_gan = Subset(dataset_gan, indices)\n",
    "\n",
    "    # Create a list to store augmented images\n",
    "    augmented_images = []\n",
    "\n",
    "    # Apply augmentations to each image in the loaded dataset\n",
    "    for i in range(len(dataset_gan)):\n",
    "        image, label = dataset_gan[i]\n",
    "\n",
    "        # Generate num_variants augmented versions of image\n",
    "        for _ in range(num_variants):\n",
    "            augmented_image = transform_gan_p(image)\n",
    "\n",
    "            augmented_images.append((augmented_image, label))\n",
    "\n",
    "    # Create new dataset with augmented images\n",
    "    augmented_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.stack([image[0] for image in augmented_images]),  # Stack all augmented images\n",
    "        torch.tensor([image[1] for image in augmented_images])  # Stack all labels\n",
    "    )\n",
    "\n",
    "    # Create DataLoader for the GAN data\n",
    "    dataloader_gan = DataLoader(augmented_dataset, batch_size = batch_size, shuffle = True, num_workers = workers)\n",
    "\n",
    "    return dataloader_gan\n",
    "\n",
    "def load_cnn_data(batch_size = 32, workers = 4):\n",
    "    # Load dataset with CNN transformations\n",
    "    dataset_cnn = datasets.ImageFolder(root=RAW_DATA_DIR, transform = transform_cnn)\n",
    "    \n",
    "    # Create DataLoader for the CNN data\n",
    "    dataloader_cnn = DataLoader(dataset_cnn, batch_size=batch_size, shuffle = True, num_workers = workers)\n",
    "\n",
    "    return dataloader_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d5c2ea",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "014a3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2af1f2",
   "metadata": {},
   "source": [
    "## Part 2: Creating the DCGAN Generator and Discriminator Classes\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "BATCH_SIZE = 128        # Number of images per training batch\n",
    "\n",
    "INPUT_DIMENSION = 100   # Dimensionality of the generator input\n",
    "\n",
    "NC = 3                  # Number of channels in the training images\n",
    "\n",
    "NGF = 64                # Base number of feature maps in the generator\n",
    "\n",
    "NDF = 64                # Base number of feature maps in Discriminator\n",
    "\n",
    "EPOCHS = 200            # Number of training epochs\n",
    "\n",
    "LEARNING_RATE = 0.0002  # Learning rate for both optimizers\n",
    "\n",
    "BETA1 = 0.5             # Beta1 value for the Adam optimizer to help stabilize DCGAN training\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "NGPU = 1  # Number of GPUs to use (0 means CPU only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6d25128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Generator network composed of a stack of transposed conv blocks\n",
    "        self.main = nn.Sequential(\n",
    "            self._block(INPUT_DIMENSION, NGF * 16, 4, 1, 0, bias = False),  # First layer: latent vector -> feature map\n",
    "            self._block(NGF * 16, NGF * 8, 4, 2, 1, bias = False),          # Upsample to 8 x 8\n",
    "            self._block(NGF * 8, NGF * 4, 4, 2, 1, bias = False),           # Upsample to 16 x 16\n",
    "            self._block(NGF * 4, NGF * 2, 4, 2, 1, bias = False),           # Upsample to 32 x 32\n",
    "            self._block(NGF * 2, NGF, 4, 2, 1, bias = False),               # Upsample to 64 x 64\n",
    "\n",
    "            nn.ConvTranspose2d(NGF, NC, 4, 2, 1, bias = False),      # Final upsample to 128x128 with RGB output\n",
    "            nn.Tanh()                                                # Output pixel values in [-1, 1]\n",
    "        )\n",
    "\n",
    "    # Helper function to define a generator block:\n",
    "\n",
    "    # ConvTranspose2d -> InstanceNorm2d -> ReLU -> Dropout\n",
    "\n",
    "    def _block(self, i_channels, o_channels, kernel_size, stride, padding, bias):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                i_channels, \n",
    "                o_channels, \n",
    "                kernel_size, \n",
    "                stride, \n",
    "                padding, \n",
    "                bias = bias),\n",
    "            nn.InstanceNorm2d(o_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout2d(0.3) # Dropout to help regularize on small data\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7581479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Discriminator network composed of downsampling conv blocks\n",
    "        self.main = nn.Sequential(\n",
    "            self._block(NC, NDF, 4, 2, 1, bias = False, use_batchNorm2D = False), # First block: no BatchNorm\n",
    "            self._block(NDF,     NDF *  2, 4, 2, 1, bias = False),                # Downsample to 32 x 32\n",
    "            self._block(NDF * 2, NDF *  4, 4, 2, 1, bias = False),                # Downsample to 16 x 16\n",
    "            self._block(NDF * 4, NDF *  8, 4, 2, 1, bias = False),                # Downsample to 8 x 8\n",
    "            self._block(NDF * 8, NDF * 16, 4, 2, 1, bias = False),                # Downsample to 4 x 4\n",
    "\n",
    "            nn.Conv2d(NDF * 16, 1, 4, 1, 0, bias = False),                        # Final layer: reduce to 1 x 1\n",
    "        )\n",
    "\n",
    "    # Helper function to define a discriminator block:\n",
    "\n",
    "    # Conv2d -> (optional) InstanceNorm2d -> LeakyReLU\n",
    "\n",
    "    def _block(self, i_channels, o_channels, kernel_size, stride, padding, bias, use_batchNorm2D = True):\n",
    "        layers = [nn.Conv2d(\n",
    "            i_channels, \n",
    "            o_channels, \n",
    "            kernel_size, \n",
    "            stride, \n",
    "            padding, \n",
    "            bias = bias)]\n",
    "        \n",
    "        if use_batchNorm2D:\n",
    "            layers.append(nn.InstanceNorm2d(o_channels))\n",
    "        \n",
    "        layers.append(nn.LeakyReLU(0.2, inplace = True))\n",
    "        layers.append(nn.Dropout2d(0.3)) # Dropout to help regularize on small data\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb85cf",
   "metadata": {},
   "source": [
    "## Part 3: Training a DCGAN for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20fa7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_OUTPUT_DIRECTORY_TEST = \"../model2/gan_test\" # for debugging while training\n",
    "\n",
    "def prepare_output_directory():\n",
    "\n",
    "    for cls in BANANA_CLASSES:\n",
    "        if cls != \"sigatoka\":\n",
    "            full_path = Path(GAN_OUTPUT_DIRECTORY_TEST) / cls\n",
    "\n",
    "            # If the directory exists, remove and recreate it\n",
    "            if full_path.exists():\n",
    "                shutil.rmtree(full_path)\n",
    "\n",
    "            full_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "prepare_output_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff876c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dcgan_per_class(target_class = None, resume = False, checkpoint_path = None):\n",
    "    if target_class:\n",
    "        dataloader = load_gan_data(batch_size = BATCH_SIZE, target_class = target_class)\n",
    "    \n",
    "    else:\n",
    "        dataloader = load_gan_data(batch_size = BATCH_SIZE)\n",
    "\n",
    "    # Initialize Generator and Discriminator\n",
    "    netG = Generator(ngpu = NGPU).to(DEVICE)\n",
    "    netD = Discriminator(ngpu = NGPU).to(DEVICE)\n",
    "\n",
    "    # Initialize weights\n",
    "    netG.apply(initialize_weights)\n",
    "    netD.apply(initialize_weights)\n",
    "\n",
    "    # Handle multi-GPU setup if applicable\n",
    "    if (DEVICE.type == \"cuda\") and (NGPU > 1):\n",
    "        netG = nn.DataParallel(netG, list(range(NGPU)))\n",
    "        netD = nn.DataParallel(netD, list(range(NGPU)))\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Fixed noise for generating sample outputs and tracking progress during training\n",
    "    fixed_noise = torch.randn(64, INPUT_DIMENSION, 1, 1, device = DEVICE)\n",
    "\n",
    "    # Optimizers for Generator and Discriminato\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr = LEARNING_RATE, betas = (BETA1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr = LEARNING_RATE, betas = (BETA1, 0.999))\n",
    "\n",
    "    # Labels for real and fake images\n",
    "    real_label = 0.9 # Slightly less than 1\n",
    "    fake_label = 0.1 # Slightly more than 0\n",
    "\n",
    "    # Default starting epoch\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Allows training models from a checkpoint para hindi paulit-ulit\n",
    "\n",
    "    if resume and checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location = DEVICE)\n",
    "        netG.load_state_dict(checkpoint[\"netG\"])\n",
    "        netD.load_state_dict(checkpoint[\"netD\"])\n",
    "        optimizerG.load_state_dict(checkpoint[\"optimizerG\"])\n",
    "        optimizerD.load_state_dict(checkpoint[\"optimizerD\"])\n",
    "\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    SAVE_EVERY = 10  # Save checkpoint every 10 epochs\n",
    "\n",
    "    # Actual training\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, (real_images, _) in enumerate(dataloader): # Iterate through batches in the dataset\n",
    "            # 1. Update Discriminator: \n",
    "            #    maximize log(D(x)) + log(1 - D(G(z)))\n",
    "\n",
    "            # 1.A. Train Discriminator on real images\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # Format real batch\n",
    "            real_images = real_images.to(DEVICE)\n",
    "\n",
    "            # Well, train the Discriminator on noisy real images\n",
    "            noise = torch.randn_like(real_images) * 0.1 # 0.1 controls the magnitude of noise\n",
    "\n",
    "            noisy_real_images = real_images + noise\n",
    "\n",
    "            size = real_images.size(0)\n",
    "\n",
    "            label = torch.full((size,), real_label, dtype = torch.float, device = DEVICE)\n",
    "\n",
    "            # Forward pass noisy real images through Discriminator\n",
    "            output = netD(noisy_real_images).view(-1)\n",
    "\n",
    "            # Calculate Discriminator loss for noisy real images\n",
    "            errD_real = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for noisy real images\n",
    "            errD_real.backward()\n",
    "\n",
    "            # Mean output for noisy real images\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # 1.B. Train Discriminator on batch of all fake images\n",
    "\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.randn(size, INPUT_DIMENSION, 1, 1, device = DEVICE)\n",
    "\n",
    "            # Generate fake images with Generator\n",
    "            fake = netG(noise)\n",
    "\n",
    "            # Classify fake images with Discriminator\n",
    "            label.fill_(fake_label)\n",
    "\n",
    "            # Forward pass fake images through Discriminator\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "\n",
    "            # Calculate Discriminator loss for fake images\n",
    "            errD_fake = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for fake images\n",
    "            errD_fake.backward()\n",
    "\n",
    "            # Clip Discriminator gradients for stability\n",
    "            torch.nn.utils.clip_grad_norm_(netD.parameters(), max_norm = 1.0)\n",
    "\n",
    "            # Mean output for fake images\n",
    "            D_G_z1 = output.mean().item()\n",
    "\n",
    "            # Compute total Discriminator error = real error + fake error\n",
    "            errD = errD_real + errD_fake\n",
    "\n",
    "            # Finally update Discriminator\n",
    "            optimizerD.step()\n",
    "\n",
    "            # 2. Update Generator: \n",
    "            #    maximize log(D(G(z)))\n",
    "\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for Generator cost\n",
    "\n",
    "            # Pass fake images through Discriminator\n",
    "            output = netD(fake).view(-1)\n",
    "\n",
    "            # Calculate Generator loss based on Discriminator's output\n",
    "            errG = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for Generator\n",
    "            errG.backward()\n",
    "\n",
    "            # Mean output for fake images after Generator update\n",
    "            D_G_z2 = output.mean().item()\n",
    "\n",
    "            # Finally update Generator\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Debugging: Print losses and monitor training progress\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(\n",
    "                  f\"Epoch [{epoch}/{EPOCHS}] Batch {i}/{len(dataloader)} \\\n",
    "                    Loss D: {errD.item():.4f}, loss G: {errG.item():.4f} \\\n",
    "                    D(x): {D_x:.4f}, \\\n",
    "                    D(G(z))_real: {D_G_z1:.4f}, D(G(z))_fake: {D_G_z2:.4f}\"\n",
    "                )\n",
    "\n",
    "        if epoch % SAVE_EVERY == 0:\n",
    "            fake_images = netG(fixed_noise).detach()\n",
    "\n",
    "            if target_class:\n",
    "                path = f\"{GAN_OUTPUT_DIRECTORY_TEST}/{target_class}\"\n",
    "            \n",
    "            else:\n",
    "                path = GAN_OUTPUT_DIRECTORY_TEST\n",
    "\n",
    "            # Save images Generator could produce during checkpoints\n",
    "            save_image(\n",
    "                fake_images,\n",
    "                os.path.join(path, f\"sample_epoch_{epoch}.png\"),\n",
    "                normalize = True\n",
    "            )\n",
    "\n",
    "            # Save model version\n",
    "            save_dict = {\n",
    "                \"epoch\": epoch,\n",
    "                \"netG\": netG.state_dict(),\n",
    "                \"netD\": netD.state_dict(),\n",
    "                \"optimizerG\": optimizerG.state_dict(),\n",
    "                \"optimizerD\": optimizerD.state_dict(),\n",
    "            }\n",
    "\n",
    "            torch.save(save_dict, os.path.join(path, f\"checkpoint_epoch_{epoch}.pth\"))\n",
    "\n",
    "    return netG, netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd2ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] Batch 0/12                     Loss D: 2.2262, loss G: 5.4314                     D(x): 0.8105,                     D(G(z))_real: 0.8789, D(G(z))_fake: -6.0181\n",
      "Epoch [1/100] Batch 0/12                     Loss D: 1.6429, loss G: 2.9868                     D(x): 1.9622,                     D(G(z))_real: -1.0652, D(G(z))_fake: -3.0778\n",
      "Epoch [2/100] Batch 0/12                     Loss D: 1.4284, loss G: 4.7325                     D(x): 2.3362,                     D(G(z))_real: -0.6259, D(G(z))_fake: -5.2165\n",
      "Epoch [3/100] Batch 0/12                     Loss D: 1.6087, loss G: 5.6744                     D(x): 3.0469,                     D(G(z))_real: -0.5782, D(G(z))_fake: -6.2853\n",
      "Epoch [4/100] Batch 0/12                     Loss D: 1.3591, loss G: 8.9547                     D(x): 4.0753,                     D(G(z))_real: -1.2167, D(G(z))_fake: -9.9492\n",
      "Epoch [5/100] Batch 0/12                     Loss D: 1.1862, loss G: 0.6901                     D(x): 3.7823,                     D(G(z))_real: -5.4778, D(G(z))_fake: 1.5656\n",
      "Epoch [6/100] Batch 0/12                     Loss D: 0.9551, loss G: 7.4197                     D(x): 3.4787,                     D(G(z))_real: -3.4106, D(G(z))_fake: -8.2401\n",
      "Epoch [7/100] Batch 0/12                     Loss D: 1.7208, loss G: 7.6694                     D(x): 4.1775,                     D(G(z))_real: 0.3605, D(G(z))_fake: -8.5207\n",
      "Epoch [8/100] Batch 0/12                     Loss D: 1.2912, loss G: 7.3549                     D(x): 4.1834,                     D(G(z))_real: -0.6774, D(G(z))_fake: -8.1709\n",
      "Epoch [9/100] Batch 0/12                     Loss D: 1.7495, loss G: 6.2145                     D(x): 3.6481,                     D(G(z))_real: 0.4380, D(G(z))_fake: -6.9007\n",
      "Epoch [10/100] Batch 0/12                     Loss D: 1.3861, loss G: 6.7780                     D(x): 3.7946,                     D(G(z))_real: -0.6481, D(G(z))_fake: -7.5292\n",
      "Epoch [11/100] Batch 0/12                     Loss D: 0.9482, loss G: 0.4374                     D(x): 3.8785,                     D(G(z))_real: -3.1733, D(G(z))_fake: 2.4695\n",
      "Epoch [12/100] Batch 0/12                     Loss D: 0.9776, loss G: 0.8707                     D(x): 3.1963,                     D(G(z))_real: -4.3544, D(G(z))_fake: 0.0764\n",
      "Epoch [13/100] Batch 0/12                     Loss D: 0.9353, loss G: 0.5433                     D(x): 2.7016,                     D(G(z))_real: -4.5488, D(G(z))_fake: 1.6523\n",
      "Epoch [14/100] Batch 0/12                     Loss D: 0.9610, loss G: 0.4298                     D(x): 3.5413,                     D(G(z))_real: -4.1920, D(G(z))_fake: 2.5291\n",
      "Epoch [15/100] Batch 0/12                     Loss D: 0.9557, loss G: 0.5186                     D(x): 2.5145,                     D(G(z))_real: -3.8826, D(G(z))_fake: 1.4319\n",
      "Epoch [16/100] Batch 0/12                     Loss D: 1.0864, loss G: 2.5305                     D(x): 2.0909,                     D(G(z))_real: -5.0942, D(G(z))_fake: -2.6688\n",
      "Epoch [17/100] Batch 0/12                     Loss D: 0.9443, loss G: 1.3491                     D(x): 2.9670,                     D(G(z))_real: -4.2939, D(G(z))_fake: -0.9845\n",
      "Epoch [18/100] Batch 0/12                     Loss D: 1.1590, loss G: 2.1302                     D(x): 2.9735,                     D(G(z))_real: -5.7468, D(G(z))_fake: -2.1090\n",
      "Epoch [19/100] Batch 0/12                     Loss D: 0.9442, loss G: 1.4539                     D(x): 3.2838,                     D(G(z))_real: -4.6946, D(G(z))_fake: -1.1247\n",
      "Epoch [20/100] Batch 0/12                     Loss D: 0.8490, loss G: 1.3613                     D(x): 2.7013,                     D(G(z))_real: -3.5212, D(G(z))_fake: -1.0258\n",
      "Epoch [21/100] Batch 0/12                     Loss D: 0.8621, loss G: 1.3045                     D(x): 2.7989,                     D(G(z))_real: -3.3983, D(G(z))_fake: -0.9273\n",
      "Epoch [22/100] Batch 0/12                     Loss D: 0.9188, loss G: 1.8504                     D(x): 2.0439,                     D(G(z))_real: -3.2273, D(G(z))_fake: -1.7051\n",
      "Epoch [23/100] Batch 0/12                     Loss D: 1.1764, loss G: 3.9756                     D(x): 2.2123,                     D(G(z))_real: -1.8257, D(G(z))_fake: -4.3797\n",
      "Epoch [24/100] Batch 0/12                     Loss D: 1.5046, loss G: 3.0166                     D(x): 1.5955,                     D(G(z))_real: -1.3988, D(G(z))_fake: -3.2332\n",
      "Epoch [25/100] Batch 0/12                     Loss D: 1.1573, loss G: 2.2024                     D(x): 1.9403,                     D(G(z))_real: -2.1897, D(G(z))_fake: -2.1275\n",
      "Epoch [26/100] Batch 0/12                     Loss D: 1.1940, loss G: 2.5172                     D(x): 1.7022,                     D(G(z))_real: -3.3821, D(G(z))_fake: -2.4972\n",
      "Epoch [27/100] Batch 0/12                     Loss D: 1.4998, loss G: 3.2839                     D(x): 1.4574,                     D(G(z))_real: -0.6080, D(G(z))_fake: -3.5657\n",
      "Epoch [28/100] Batch 0/12                     Loss D: 1.1071, loss G: 2.5941                     D(x): 1.6805,                     D(G(z))_real: -2.1898, D(G(z))_fake: -2.6558\n",
      "Epoch [29/100] Batch 0/12                     Loss D: 1.1392, loss G: 2.2389                     D(x): 1.2519,                     D(G(z))_real: -2.7057, D(G(z))_fake: -2.1394\n",
      "Epoch [30/100] Batch 0/12                     Loss D: 1.0148, loss G: 2.8180                     D(x): 2.2216,                     D(G(z))_real: -1.6129, D(G(z))_fake: -3.0103\n",
      "Epoch [31/100] Batch 0/12                     Loss D: 1.2238, loss G: 3.0680                     D(x): 2.2052,                     D(G(z))_real: -1.4379, D(G(z))_fake: -3.2985\n",
      "Epoch [32/100] Batch 0/12                     Loss D: 0.9895, loss G: 2.3844                     D(x): 1.8200,                     D(G(z))_real: -2.2871, D(G(z))_fake: -2.4538\n",
      "Epoch [33/100] Batch 0/12                     Loss D: 1.0086, loss G: 2.3607                     D(x): 1.3672,                     D(G(z))_real: -2.9597, D(G(z))_fake: -2.3921\n",
      "Epoch [34/100] Batch 0/12                     Loss D: 0.9197, loss G: 2.7192                     D(x): 2.7336,                     D(G(z))_real: -1.8380, D(G(z))_fake: -2.8884\n",
      "Epoch [35/100] Batch 0/12                     Loss D: 0.9069, loss G: 3.2194                     D(x): 3.0414,                     D(G(z))_real: -2.1936, D(G(z))_fake: -3.5068\n",
      "Epoch [36/100] Batch 0/12                     Loss D: 1.0125, loss G: 2.8207                     D(x): 2.3857,                     D(G(z))_real: -1.4322, D(G(z))_fake: -2.9910\n",
      "Epoch [37/100] Batch 0/12                     Loss D: 1.0750, loss G: 2.7629                     D(x): 2.2468,                     D(G(z))_real: -1.4521, D(G(z))_fake: -2.9415\n",
      "Epoch [38/100] Batch 0/12                     Loss D: 0.9059, loss G: 3.2857                     D(x): 2.9721,                     D(G(z))_real: -2.0159, D(G(z))_fake: -3.5670\n",
      "Epoch [39/100] Batch 0/12                     Loss D: 0.9003, loss G: 2.9204                     D(x): 3.0256,                     D(G(z))_real: -2.1600, D(G(z))_fake: -3.1635\n",
      "Epoch [40/100] Batch 0/12                     Loss D: 1.0090, loss G: 1.7696                     D(x): 1.7375,                     D(G(z))_real: -2.4187, D(G(z))_fake: -1.6116\n",
      "Epoch [41/100] Batch 0/12                     Loss D: 0.9301, loss G: 1.9624                     D(x): 1.5718,                     D(G(z))_real: -2.1972, D(G(z))_fake: -1.9416\n",
      "Epoch [42/100] Batch 0/12                     Loss D: 0.8944, loss G: 2.1152                     D(x): 1.8775,                     D(G(z))_real: -2.3441, D(G(z))_fake: -2.1087\n",
      "Epoch [43/100] Batch 0/12                     Loss D: 0.8757, loss G: 3.5055                     D(x): 2.5302,                     D(G(z))_real: -2.1262, D(G(z))_fake: -3.8420\n",
      "Epoch [44/100] Batch 0/12                     Loss D: 0.9087, loss G: 2.5657                     D(x): 2.5281,                     D(G(z))_real: -2.2350, D(G(z))_fake: -2.6721\n",
      "Epoch [45/100] Batch 0/12                     Loss D: 0.9339, loss G: 2.1216                     D(x): 1.9628,                     D(G(z))_real: -2.3338, D(G(z))_fake: -2.0781\n",
      "Epoch [46/100] Batch 0/12                     Loss D: 0.8783, loss G: 2.7449                     D(x): 2.0874,                     D(G(z))_real: -2.2329, D(G(z))_fake: -2.9279\n",
      "Epoch [47/100] Batch 0/12                     Loss D: 0.8958, loss G: 3.6854                     D(x): 2.2375,                     D(G(z))_real: -2.1590, D(G(z))_fake: -4.0511\n",
      "Epoch [48/100] Batch 0/12                     Loss D: 0.9731, loss G: 3.1000                     D(x): 2.7840,                     D(G(z))_real: -1.3688, D(G(z))_fake: -3.3656\n",
      "Epoch [49/100] Batch 0/12                     Loss D: 1.0557, loss G: 2.7012                     D(x): 2.6163,                     D(G(z))_real: -0.8958, D(G(z))_fake: -2.8780\n",
      "Epoch [50/100] Batch 0/12                     Loss D: 0.9511, loss G: 2.8519                     D(x): 1.9777,                     D(G(z))_real: -1.9112, D(G(z))_fake: -3.0597\n",
      "Epoch [51/100] Batch 0/12                     Loss D: 1.0295, loss G: 2.5820                     D(x): 2.8955,                     D(G(z))_real: -1.2225, D(G(z))_fake: -2.7231\n",
      "Epoch [52/100] Batch 0/12                     Loss D: 1.0362, loss G: 2.2953                     D(x): 1.7333,                     D(G(z))_real: -2.1558, D(G(z))_fake: -2.3767\n",
      "Epoch [53/100] Batch 0/12                     Loss D: 0.9422, loss G: 3.1405                     D(x): 2.2156,                     D(G(z))_real: -1.7330, D(G(z))_fake: -3.4277\n",
      "Epoch [54/100] Batch 0/12                     Loss D: 0.9070, loss G: 2.2575                     D(x): 1.8230,                     D(G(z))_real: -2.5807, D(G(z))_fake: -2.3252\n",
      "Epoch [55/100] Batch 0/12                     Loss D: 0.9742, loss G: 2.6458                     D(x): 3.4084,                     D(G(z))_real: -1.1459, D(G(z))_fake: -2.8079\n",
      "Epoch [56/100] Batch 0/12                     Loss D: 1.0100, loss G: 2.4129                     D(x): 3.0600,                     D(G(z))_real: -1.0749, D(G(z))_fake: -2.5230\n",
      "Epoch [57/100] Batch 0/12                     Loss D: 1.0071, loss G: 2.5721                     D(x): 2.4940,                     D(G(z))_real: -1.0492, D(G(z))_fake: -2.7416\n",
      "Epoch [58/100] Batch 0/12                     Loss D: 0.8864, loss G: 2.8413                     D(x): 1.9845,                     D(G(z))_real: -2.1873, D(G(z))_fake: -3.0563\n",
      "Epoch [59/100] Batch 0/12                     Loss D: 0.9154, loss G: 2.8663                     D(x): 2.8772,                     D(G(z))_real: -1.5373, D(G(z))_fake: -3.0721\n",
      "Epoch [60/100] Batch 0/12                     Loss D: 0.8329, loss G: 2.0661                     D(x): 2.0994,                     D(G(z))_real: -2.2090, D(G(z))_fake: -2.0641\n",
      "Epoch [61/100] Batch 0/12                     Loss D: 0.8745, loss G: 1.7539                     D(x): 1.6630,                     D(G(z))_real: -2.6097, D(G(z))_fake: -1.6016\n",
      "Epoch [62/100] Batch 0/12                     Loss D: 0.8493, loss G: 2.8601                     D(x): 2.9706,                     D(G(z))_real: -1.6334, D(G(z))_fake: -3.0821\n",
      "Epoch [63/100] Batch 0/12                     Loss D: 1.3040, loss G: 1.6963                     D(x): 3.5677,                     D(G(z))_real: -0.1037, D(G(z))_fake: -1.5534\n",
      "Epoch [64/100] Batch 0/12                     Loss D: 1.0314, loss G: 2.9975                     D(x): 1.9199,                     D(G(z))_real: -1.2578, D(G(z))_fake: -3.2310\n",
      "Epoch [65/100] Batch 0/12                     Loss D: 0.9020, loss G: 2.8391                     D(x): 2.1953,                     D(G(z))_real: -2.1757, D(G(z))_fake: -3.0507\n",
      "Epoch [66/100] Batch 0/12                     Loss D: 0.9357, loss G: 2.0257                     D(x): 1.6724,                     D(G(z))_real: -2.2784, D(G(z))_fake: -2.0065\n",
      "Epoch [67/100] Batch 0/12                     Loss D: 0.8771, loss G: 2.7967                     D(x): 2.1302,                     D(G(z))_real: -1.9360, D(G(z))_fake: -3.0012\n",
      "Epoch [68/100] Batch 0/12                     Loss D: 0.9844, loss G: 2.9451                     D(x): 1.5295,                     D(G(z))_real: -1.6502, D(G(z))_fake: -3.1925\n",
      "Epoch [69/100] Batch 0/12                     Loss D: 0.9330, loss G: 1.7871                     D(x): 1.7307,                     D(G(z))_real: -2.0588, D(G(z))_fake: -1.6901\n",
      "Epoch [70/100] Batch 0/12                     Loss D: 0.8818, loss G: 2.1584                     D(x): 1.9254,                     D(G(z))_real: -2.0999, D(G(z))_fake: -2.1731\n",
      "Epoch [71/100] Batch 0/12                     Loss D: 0.8581, loss G: 3.7731                     D(x): 2.5409,                     D(G(z))_real: -1.8793, D(G(z))_fake: -4.1608\n",
      "Epoch [72/100] Batch 0/12                     Loss D: 0.9659, loss G: 3.1626                     D(x): 2.8566,                     D(G(z))_real: -1.0939, D(G(z))_fake: -3.4502\n",
      "Epoch [73/100] Batch 0/12                     Loss D: 0.9297, loss G: 2.2114                     D(x): 1.3545,                     D(G(z))_real: -2.2910, D(G(z))_fake: -2.2387\n",
      "Epoch [74/100] Batch 0/12                     Loss D: 1.2005, loss G: 2.8392                     D(x): 3.6704,                     D(G(z))_real: -0.3630, D(G(z))_fake: -3.0237\n",
      "Epoch [75/100] Batch 0/12                     Loss D: 0.9210, loss G: 3.3501                     D(x): 2.6178,                     D(G(z))_real: -1.3289, D(G(z))_fake: -3.6739\n",
      "Epoch [76/100] Batch 0/12                     Loss D: 1.0001, loss G: 3.6578                     D(x): 1.7740,                     D(G(z))_real: -1.2580, D(G(z))_fake: -4.0255\n",
      "Epoch [77/100] Batch 0/12                     Loss D: 0.8665, loss G: 2.6991                     D(x): 1.8930,                     D(G(z))_real: -2.1796, D(G(z))_fake: -2.8921\n",
      "Epoch [78/100] Batch 0/12                     Loss D: 0.8775, loss G: 2.0348                     D(x): 1.5880,                     D(G(z))_real: -2.4349, D(G(z))_fake: -2.0526\n",
      "Epoch [79/100] Batch 0/12                     Loss D: 1.0599, loss G: 2.4946                     D(x): 2.9145,                     D(G(z))_real: -0.9830, D(G(z))_fake: -2.6220\n",
      "Epoch [80/100] Batch 0/12                     Loss D: 1.0786, loss G: 3.3317                     D(x): 1.7799,                     D(G(z))_real: -0.9071, D(G(z))_fake: -3.6454\n",
      "Epoch [81/100] Batch 0/12                     Loss D: 1.1312, loss G: 1.9743                     D(x): 2.6598,                     D(G(z))_real: -0.4722, D(G(z))_fake: -1.9314\n",
      "Epoch [82/100] Batch 0/12                     Loss D: 1.0032, loss G: 2.0958                     D(x): 1.6377,                     D(G(z))_real: -1.6780, D(G(z))_fake: -2.1115\n",
      "Epoch [83/100] Batch 0/12                     Loss D: 0.8522, loss G: 2.5373                     D(x): 1.9298,                     D(G(z))_real: -2.5437, D(G(z))_fake: -2.6876\n",
      "Epoch [84/100] Batch 0/12                     Loss D: 1.0104, loss G: 2.5970                     D(x): 2.0476,                     D(G(z))_real: -1.1701, D(G(z))_fake: -2.7468\n",
      "Epoch [85/100] Batch 0/12                     Loss D: 0.8757, loss G: 2.9636                     D(x): 2.0672,                     D(G(z))_real: -1.5571, D(G(z))_fake: -3.2260\n",
      "Epoch [86/100] Batch 0/12                     Loss D: 0.9330, loss G: 2.5511                     D(x): 2.6820,                     D(G(z))_real: -1.2084, D(G(z))_fake: -2.7206\n",
      "Epoch [87/100] Batch 0/12                     Loss D: 0.8970, loss G: 2.4871                     D(x): 2.2102,                     D(G(z))_real: -1.3345, D(G(z))_fake: -2.6310\n",
      "Epoch [88/100] Batch 0/12                     Loss D: 0.8616, loss G: 2.1833                     D(x): 1.7305,                     D(G(z))_real: -2.7427, D(G(z))_fake: -2.2399\n",
      "Epoch [89/100] Batch 0/12                     Loss D: 0.9617, loss G: 2.7939                     D(x): 1.7558,                     D(G(z))_real: -1.6333, D(G(z))_fake: -2.9923\n",
      "Epoch [90/100] Batch 0/12                     Loss D: 0.8756, loss G: 2.6766                     D(x): 2.5421,                     D(G(z))_real: -1.4688, D(G(z))_fake: -2.8637\n",
      "Epoch [91/100] Batch 0/12                     Loss D: 0.8325, loss G: 2.8951                     D(x): 2.1095,                     D(G(z))_real: -2.0053, D(G(z))_fake: -3.1304\n",
      "Epoch [92/100] Batch 0/12                     Loss D: 0.9099, loss G: 2.3880                     D(x): 1.6245,                     D(G(z))_real: -1.8060, D(G(z))_fake: -2.4932\n",
      "Epoch [93/100] Batch 0/12                     Loss D: 1.0150, loss G: 2.5721                     D(x): 2.0873,                     D(G(z))_real: -1.0095, D(G(z))_fake: -2.7284\n",
      "Epoch [94/100] Batch 0/12                     Loss D: 0.8897, loss G: 2.5137                     D(x): 1.9936,                     D(G(z))_real: -1.4863, D(G(z))_fake: -2.6621\n",
      "Epoch [95/100] Batch 0/12                     Loss D: 0.8673, loss G: 2.7229                     D(x): 2.0433,                     D(G(z))_real: -1.6040, D(G(z))_fake: -2.9125\n",
      "Epoch [96/100] Batch 0/12                     Loss D: 0.8639, loss G: 2.0508                     D(x): 1.9825,                     D(G(z))_real: -2.5184, D(G(z))_fake: -2.0351\n",
      "Epoch [97/100] Batch 0/12                     Loss D: 0.9661, loss G: 2.9253                     D(x): 1.6923,                     D(G(z))_real: -1.3279, D(G(z))_fake: -3.1655\n",
      "Epoch [98/100] Batch 0/12                     Loss D: 0.8941, loss G: 1.7106                     D(x): 1.4540,                     D(G(z))_real: -1.9824, D(G(z))_fake: -1.6007\n",
      "Epoch [99/100] Batch 0/12                     Loss D: 0.9346, loss G: 2.6565                     D(x): 1.5869,                     D(G(z))_real: -1.8707, D(G(z))_fake: -2.8526\n"
     ]
    }
   ],
   "source": [
    "# Train DCGAN for \"cordana\" class\n",
    "trained_generator_cordana, _ = train_dcgan_per_class(target_class = \"cordana\")\n",
    "\n",
    "# train_dcgan_per_class(target_class = \"cordana\", resume = True, checkpoint_path = \"<insert>/checkpoint_epoch_<insert>.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe98212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DCGAN for \"healthy\" class\n",
    "# trained_generator_healthy, _ = train_dcgan_per_class(target_class = \"healthy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DCGAN for \"pestalotiopsis\" class\n",
    "# trained_generator_pestalotiopsis, _ = train_dcgan_per_class(target_class = \"pestalotiopsis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3e960",
   "metadata": {},
   "source": [
    "## Part 4: Generating Images for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b202ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_images(dcgan_generator, amount_to_generate, class_label, output_directory):\n",
    "\n",
    "    # Set the generator to evaluation mode to disable Dropout and InstanceNorm2d updates\n",
    "    dcgan_generator.eval()\n",
    "\n",
    "    # Construct the path to the class-specific output directory\n",
    "    class_output_directory = os.path.join(output_directory, class_label)\n",
    "\n",
    "    # Create the output directory if it does not exist just in case\n",
    "    os.makedirs(class_output_directory, exist_ok = True)\n",
    "\n",
    "    # Disable gradient computation for efficiency during inference\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, amount_to_generate, 16): # Batches of 16\n",
    "            batch_size = min(16, amount_to_generate - i) # Adjusts batch size if near the end of generation\n",
    "\n",
    "            # Sample random noise vectors as generator input\n",
    "            noise = torch.randn(batch_size, INPUT_DIMENSION, 1, 1, device = DEVICE)\n",
    "\n",
    "            # Generate a batch of fake images from the noise\n",
    "            fake = dcgan_generator(noise)\n",
    "\n",
    "            # Save each generated image to the output directory\n",
    "            for j in range(batch_size):\n",
    "                save_image(\n",
    "                    fake[j], # Single image tensor\n",
    "                    os.path.join(class_output_directory, f\"gen_{i + j}.png\"),\n",
    "                    normalize = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_OUTPUT_DIRECTORY_BALANCED = \"../model2/balanced\"\n",
    "\n",
    "# Target count based on the dominant Sigatoka class\n",
    "TARGET_COUNT = 424\n",
    "\n",
    "# Dictionary of class names and their real image counts\n",
    "real_image_counts = {\n",
    "    \"cordana\"        : 145,\n",
    "    \"healthy\"        : 115,\n",
    "    \"pestalotiopsis\" : 155,\n",
    "}\n",
    "\n",
    "# Dictionary mapping class labels to their corresponding trained generators\n",
    "trained_generators = {\n",
    "    \"cordana\"        : trained_generator_cordana,\n",
    "    \"healthy\"        : trained_generator_healthy,\n",
    "    \"pestalotiopsis\" : trained_generator_pestalotiopsis,\n",
    "}\n",
    "\n",
    "# Generate synthetic images for each underrepresented class\n",
    "for label, real_count in real_image_counts.items():\n",
    "    amount_to_generate = TARGET_COUNT - real_count\n",
    "\n",
    "    generator = trained_generators[label]\n",
    "\n",
    "    generate_synthetic_images(\n",
    "        dcgan_generator = generator, amount_to_generate = amount_to_generate, class_label = label, output_directory = GAN_OUTPUT_DIRECTORY_BALANCED\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
