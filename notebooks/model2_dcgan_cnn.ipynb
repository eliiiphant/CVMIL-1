{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88485067",
   "metadata": {},
   "source": [
    "## Importing Needed Libraries\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4359eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Data Preprocessing\n",
    "\n",
    "import torch\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Part 2: Creating the DCGAN Generator and Discriminator Classes\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Part 3: Training a DCGAN for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from numpy import cos, pi\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "\n",
    "# https://docs.pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "# https://pyimagesearch.com/2021/10/25/training-a-dcgan-in-pytorch/\n",
    "# https://medium.com/@manoharmanok/implementing-dcgan-in-pytorch-using-the-celeba-dataset-a-comprehensive-guide-660e6e8e29d2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d2cb2",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "411d5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "RAW_DATA_DIR = \"../training_data\"\n",
    "GAN_SIZE = (128, 128)\n",
    "CNN_SIZE = (224, 224)\n",
    "BANANA_CLASSES  = [\"cordana\", \"healthy\", \"pestalotiopsis\", \"sigatoka\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89358d3e",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12dfc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234d1bd",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8812e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_gan_b = transforms.Compose([\n",
    "    transforms.Resize(GAN_SIZE), # Resize for DCGAN\n",
    "    transforms.ToTensor(),       # To tensor\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5], \n",
    "        [0.5, 0.5, 0.5],\n",
    "    )  # Normalize to [-1, 1] for DCGAN\n",
    "])\n",
    "\n",
    "transform_gan_p = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(GAN_SIZE, scale = (0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomApply([ # maybe remove RandomApply for reproducibility?\n",
    "        transforms.ColorJitter(\n",
    "            brightness = 0.2, \n",
    "            contrast   = 0.2, \n",
    "            saturation = 0.2, \n",
    "            hue        = 0.05,\n",
    "        ),\n",
    "    ], p = 0.7),\n",
    "    transforms.RandomApply([ # maybe remove RandomApply for reproducibility?\n",
    "        transforms.RandomAffine(\n",
    "            degrees   = 10, \n",
    "            translate = (0.1, 0.1), \n",
    "            scale     = (0.9, 1.0), \n",
    "        ),\n",
    "    ], p = 0.7),\n",
    "])\n",
    "\n",
    "transform_cnn = transforms.Compose([\n",
    "    transforms.Resize(CNN_SIZE), # Resize for CNN\n",
    "    transforms.ToTensor(),       # To tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee0ae5",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53e02535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gan_data(batch_size = 32, workers = 4, target_class = None, num_variants = 10, seed = 42, directory = RAW_DATA_DIR):\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    # Load full dataset with base GAN transformations\n",
    "    dataset_gan = datasets.ImageFolder(root = directory, transform = transform_gan_b)\n",
    "\n",
    "    if target_class:\n",
    "        # Get class index from the class name\n",
    "        class_index = dataset_gan.class_to_idx[target_class]\n",
    "\n",
    "        # Filter indices where target matches\n",
    "        indices = [i for i, (_, label) in enumerate(dataset_gan.samples) if label == class_index]\n",
    "\n",
    "        # Wrap in a Subset\n",
    "        dataset_gan = Subset(dataset_gan, indices)\n",
    "\n",
    "        # Create a list to store augmented images\n",
    "        augmented_images = []\n",
    "\n",
    "        rng = random.Random(seed)\n",
    "\n",
    "        # Apply augmentations to each image in the loaded dataset\n",
    "        for i in range(len(dataset_gan)):\n",
    "            image, label = dataset_gan[i]\n",
    "\n",
    "            # Generate num_variants augmented versions of image\n",
    "            for _ in range(num_variants):\n",
    "                torch.manual_seed(rng.randint(0, 999999))\n",
    "        \n",
    "                augmented_image = transform_gan_p(image)\n",
    "\n",
    "                augmented_images.append((augmented_image, label))\n",
    "\n",
    "        # Create new dataset with augmented images\n",
    "        final_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.stack([image for image, _ in augmented_images]),  # Stack all augmented images\n",
    "            torch.tensor([label for _, label in augmented_images])  # Stack all labels\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        final_dataset = dataset_gan\n",
    "\n",
    "    # Create DataLoader for the GAN data\n",
    "    dataloader_gan = DataLoader(final_dataset, batch_size = batch_size, shuffle = True, num_workers = workers, generator = generator)\n",
    "\n",
    "    return dataloader_gan\n",
    "\n",
    "def load_cnn_data(batch_size = 32, workers = 4):\n",
    "    # Load dataset with CNN transformations\n",
    "    dataset_cnn = datasets.ImageFolder(root=RAW_DATA_DIR, transform = transform_cnn)\n",
    "    \n",
    "    # Create DataLoader for the CNN data\n",
    "    dataloader_cnn = DataLoader(dataset_cnn, batch_size=batch_size, shuffle = True, num_workers = workers)\n",
    "\n",
    "    return dataloader_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2af1f2",
   "metadata": {},
   "source": [
    "## Part 2: Creating the DCGAN Generator and Discriminator Classes\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33db2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "SEED_NUM = 42            # Seed number for reproducibility\n",
    "\n",
    "BATCH_SIZE = 128         # Number of images per training batch\n",
    "\n",
    "INPUT_DIMENSION = 100    # Dimensionality of the generator input\n",
    "\n",
    "NC = 3                   # Number of channels in the training images\n",
    "\n",
    "NGF = 64                 # Base number of feature maps in the Generator\n",
    "\n",
    "NDF = 64                 # Base number of feature maps in the Discriminator\n",
    "\n",
    "EPOCHS = 200             # Number of training epochs\n",
    "\n",
    "CHECKPOINT = 5           # Checkpoint number for model saving\n",
    "\n",
    "LEARNING_RATE_G = 0.0002 # Learning rate for the Generator\n",
    "\n",
    "LEARNING_RATE_D = 0.0001 # Learning rate for the Discriminator\n",
    "\n",
    "BETA1 = 0.5              # Beta1 value for the Adam optimizer to help stabilize DCGAN training\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "NGPU = 1  # Number of GPUs to use (0 means CPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6d25128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Generator network composed of a stack of transposed conv blocks\n",
    "        self.main = nn.Sequential(\n",
    "            self._block(INPUT_DIMENSION, NGF * 16, 4, 1, 0, bias = False),  # First layer: latent vector -> feature map\n",
    "            self._block(NGF * 16, NGF * 8, 4, 2, 1, bias = False),          # Upsample to 8 x 8\n",
    "            self._block(NGF * 8, NGF * 4, 4, 2, 1, bias = False),           # Upsample to 16 x 16\n",
    "            self._block(NGF * 4, NGF * 2, 4, 2, 1, bias = False),           # Upsample to 32 x 32\n",
    "            self._block(NGF * 2, NGF, 4, 2, 1, bias = False),               # Upsample to 64 x 64\n",
    "\n",
    "            nn.ConvTranspose2d(NGF, NC, 4, 2, 1, bias = False),             # Final upsample to 128 x 128 with RGB output\n",
    "            nn.Tanh()                                                       # Output pixel values in [-1, 1]\n",
    "        )\n",
    "\n",
    "    # Helper function to define a generator block:\n",
    "\n",
    "    # ConvTranspose2d -> InstanceNorm2d -> ReLU -> Dropout\n",
    "\n",
    "    def _block(self, i_channels, o_channels, kernel_size, stride, padding, bias):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                i_channels, \n",
    "                o_channels, \n",
    "                kernel_size, \n",
    "                stride, \n",
    "                padding, \n",
    "                bias = bias),\n",
    "            nn.InstanceNorm2d(o_channels, affine = True),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3) # Dropout to help regularize on small data\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7581479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Discriminator network composed of downsampling conv blocks\n",
    "        self.main = nn.Sequential(\n",
    "            self._block(NC, NDF, 4, 2, 1, bias = False, use_instanceNorm2d = False), # First block: no InstanceNorm2d\n",
    "            self._block(NDF, NDF *  2, 4, 2, 1, bias = False),                       # Downsample to 32 x 32\n",
    "            self._block(NDF * 2, NDF *  4, 4, 2, 1, bias = False),                   # Downsample to 16 x 16\n",
    "            self._block(NDF * 4, NDF *  8, 4, 2, 1, bias = False),                   # Downsample to 8 x 8\n",
    "            self._block(NDF * 8, NDF * 16, 4, 2, 1, bias = False),                   # Downsample to 4 x 4\n",
    "\n",
    "            nn.Conv2d(NDF * 16, 1, 4, 1, 0, bias = False),                           # Final layer: reduce to 1 x 1\n",
    "        )\n",
    "\n",
    "    # Helper function to define a discriminator block:\n",
    "\n",
    "    # Conv2d -> (optional) InstanceNorm2d -> LeakyReLU\n",
    "\n",
    "    def _block(self, i_channels, o_channels, kernel_size, stride, padding, bias, use_instanceNorm2d = True):\n",
    "        layers = [nn.Conv2d(\n",
    "            i_channels, \n",
    "            o_channels, \n",
    "            kernel_size, \n",
    "            stride, \n",
    "            padding, \n",
    "            bias = bias)]\n",
    "        \n",
    "        if use_instanceNorm2d:\n",
    "            layers.append(nn.InstanceNorm2d(o_channels, affine = True))\n",
    "        \n",
    "        layers.append(nn.LeakyReLU(0.2, inplace = True))\n",
    "        layers.append(nn.Dropout(0.3)) # Dropout to help regularize on small data\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb85cf",
   "metadata": {},
   "source": [
    "## Part 3: Training a DCGAN for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "014a3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find(\"InstanceNorm\") != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99b790",
   "metadata": {},
   "source": [
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20fa7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_OUTPUT_DIRECTORY_TEST = \"../model2/gan_test\" # for debugging while training\n",
    "\n",
    "def prepare_output_directory(resume = False):\n",
    "    for cls in [\"base\"] + BANANA_CLASSES:\n",
    "        if cls != \"sigatoka\":\n",
    "            full_path = Path(GAN_OUTPUT_DIRECTORY_TEST) / cls\n",
    "\n",
    "            # Only remove and recreate the directory if not resuming\n",
    "            if not resume and full_path.exists():\n",
    "                shutil.rmtree(full_path)\n",
    "\n",
    "            full_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# prepare_output_directory(resume = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff876c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dcgan(target_class = None, resume = False, checkpoint_path = None, balanced_path = None):\n",
    "    # Set random seed for reproducibility\n",
    "    set_seed(SEED_NUM)\n",
    "\n",
    "    # Load dataset and define save path\n",
    "    if balanced_path:  \n",
    "        save_path = \"../model2/final_dcgan\"\n",
    "\n",
    "        dataloader = load_gan_data(batch_size = BATCH_SIZE, directory = balanced_path)\n",
    "\n",
    "    else:\n",
    "        save_path = GAN_OUTPUT_DIRECTORY_TEST\n",
    "\n",
    "        if target_class:\n",
    "            dataloader = load_gan_data(batch_size = BATCH_SIZE, target_class = target_class)\n",
    "\n",
    "        else:\n",
    "            dataloader = load_gan_data(batch_size = BATCH_SIZE)\n",
    "\n",
    "    # Initialize Generator and Discriminator\n",
    "    netG = Generator(ngpu = NGPU).to(DEVICE)\n",
    "    netD = Discriminator(ngpu = NGPU).to(DEVICE)\n",
    "\n",
    "    # Handle multi-GPU setup if applicable\n",
    "    if (DEVICE.type == \"cuda\") and (NGPU > 1):\n",
    "        netG = nn.DataParallel(netG, list(range(NGPU)))\n",
    "        netD = nn.DataParallel(netD, list(range(NGPU)))\n",
    "\n",
    "    # Fixed noise for generating sample outputs and tracking progress during training\n",
    "    fixed_generator = torch.Generator(device=DEVICE).manual_seed(SEED_NUM)\n",
    "\n",
    "    fixed_noise = torch.randn(64, INPUT_DIMENSION, 1, 1, device = DEVICE, generator = fixed_generator)\n",
    "\n",
    "    # Optimizers for Generator and Discriminator\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr = LEARNING_RATE_D, betas = (BETA1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr = LEARNING_RATE_G, betas = (BETA1, 0.999))\n",
    "\n",
    "    # Schedulers for optimizers\n",
    "    schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerD, T_max = EPOCHS, eta_min = 1e-6)\n",
    "    schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(optimizerG, T_max = EPOCHS, eta_min = 1e-6)\n",
    "\n",
    "    # Default starting epoch\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Loads checkpoint dcgan if provided \n",
    "\n",
    "    # Case 1: train base dcgan from a checkpoint\n",
    "    # Case 2: train per-class dcgan from scratch with base dcgan weights as a starting point\n",
    "    # Case 3: train per-class dcgan from a checkpoint\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location = DEVICE)\n",
    "        netG.load_state_dict(checkpoint[\"netG\"])\n",
    "        netD.load_state_dict(checkpoint[\"netD\"])\n",
    "\n",
    "        # Loads checkpoint optimizers if resuming training\n",
    "        if resume:\n",
    "            optimizerG.load_state_dict(checkpoint[\"optimizerG\"])\n",
    "            optimizerD.load_state_dict(checkpoint[\"optimizerD\"])\n",
    "            start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "    # Fresh start\n",
    "    else:\n",
    "        netG.apply(initialize_weights)\n",
    "        netD.apply(initialize_weights)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Labels for real and fake images\n",
    "    real_label = 0.9 # Slightly less than 1\n",
    "    fake_label = 0.1 # Slightly more than 0\n",
    "\n",
    "    # Actual training\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        # Calculate noise magnitude decay\n",
    "        noise_magnitude = 0.1 * 0.5 * (1 + cos(pi * epoch / EPOCHS)) # 0.1 = maximum noise magnitude\n",
    "    \n",
    "        for i, (real_images, _) in enumerate(dataloader): # Iterate through batches in the dataset\n",
    "            # 1. Update Discriminator: \n",
    "            #    maximize log(D(x)) + log(1 - D(G(z)))\n",
    "\n",
    "            # 1.A. Train Discriminator on real images\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # Format real batch\n",
    "            real_images = real_images.to(DEVICE)\n",
    "\n",
    "            # Well, train the Discriminator on noisy real images\n",
    "            noise = torch.randn_like(real_images) * noise_magnitude # noise magnitude decays\n",
    "\n",
    "            noisy_real_images = real_images + noise\n",
    "\n",
    "            size = real_images.size(0)\n",
    "\n",
    "            label = torch.full((size,), real_label, dtype = torch.float, device = DEVICE)\n",
    "\n",
    "            # Forward pass noisy real images through Discriminator\n",
    "            output = netD(noisy_real_images).view(-1)\n",
    "\n",
    "            # Calculate Discriminator loss for noisy real images\n",
    "            errD_real = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for noisy real images\n",
    "            errD_real.backward()\n",
    "\n",
    "            # Mean output for noisy real images\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # 1.B. Train Discriminator on batch of all fake images\n",
    "\n",
    "            # Generate batch of latent vectors\n",
    "            generator = torch.Generator(device=DEVICE).manual_seed(SEED_NUM + epoch)\n",
    "\n",
    "            noise = torch.randn(size, INPUT_DIMENSION, 1, 1, device = DEVICE, generator = generator)\n",
    "\n",
    "            # Generate fake images with Generator\n",
    "            fake = netG(noise)\n",
    "\n",
    "            # Classify fake images with Discriminator\n",
    "            label.fill_(fake_label)\n",
    "\n",
    "            # Forward pass fake images through Discriminator\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "\n",
    "            # Calculate Discriminator loss for fake images\n",
    "            errD_fake = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for fake images\n",
    "            errD_fake.backward()\n",
    "\n",
    "            # Clip Discriminator gradients for stability\n",
    "            torch.nn.utils.clip_grad_norm_(netD.parameters(), max_norm = 1.0)\n",
    "\n",
    "            # Mean output for fake images\n",
    "            D_G_z1 = output.mean().item()\n",
    "\n",
    "            # Compute total Discriminator error = real error + fake error\n",
    "            errD = errD_real + errD_fake\n",
    "\n",
    "            # Finally update Discriminator\n",
    "            optimizerD.step()\n",
    "\n",
    "            # 2. Update Generator: \n",
    "            #    maximize log(D(G(z)))\n",
    "\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for Generator cost\n",
    "\n",
    "            # Pass fake images through Discriminator\n",
    "            output = netD(fake).view(-1)\n",
    "\n",
    "            # Calculate Generator loss based on Discriminator's output\n",
    "            errG = criterion(output, label)\n",
    "\n",
    "            # Backpropagate error for Generator\n",
    "            errG.backward()\n",
    "\n",
    "            # Mean output for fake images after Generator update\n",
    "            D_G_z2 = output.mean().item()\n",
    "\n",
    "            # Finally update Generator\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Debugging: Print losses and monitor training progress\n",
    "\n",
    "            if i in [0, len(dataloader)//2]:\n",
    "                print(\n",
    "                  f\"Epoch [{epoch}/{EPOCHS}] Batch {i}/{len(dataloader)} \\\n",
    "                    Loss D: {errD.item():.4f}, loss G: {errG.item():.4f} \\\n",
    "                    D(x): {D_x:.4f}, \\\n",
    "                    D(G(z))_real: {D_G_z1:.4f}, D(G(z))_fake: {D_G_z2:.4f}\"\n",
    "                )\n",
    "\n",
    "        # Step learning rate schedulers\n",
    "        schedulerD.step()\n",
    "        schedulerG.step()\n",
    "\n",
    "        if epoch % CHECKPOINT == 0:\n",
    "            fake_images = netG(fixed_noise).detach()\n",
    "\n",
    "            if target_class:\n",
    "                path = f\"{save_path}/{target_class}\"\n",
    "            \n",
    "            else:\n",
    "                # root folder of save path, else /base if base dcgan\n",
    "                path = f\"{save_path}{\"\" if balanced_path else \"/base\"}\"\n",
    "\n",
    "            # Save images Generator could produce during checkpoints\n",
    "            save_image(\n",
    "                fake_images,\n",
    "                os.path.join(path, f\"sample_epoch_{epoch}.png\"),\n",
    "                normalize = True\n",
    "            )\n",
    "\n",
    "            # Save model version\n",
    "            save_dict = {\n",
    "                \"epoch\": epoch,\n",
    "                \"netG\": netG.state_dict(),\n",
    "                \"netD\": netD.state_dict(),\n",
    "                \"optimizerG\": optimizerG.state_dict(),\n",
    "                \"optimizerD\": optimizerD.state_dict(),\n",
    "            }\n",
    "\n",
    "            torch.save(save_dict, os.path.join(path, f\"checkpoint_epoch_{epoch}.pth\"))\n",
    "\n",
    "    torch.save({\n",
    "        \"epoch\": EPOCHS,\n",
    "        \"netG\": netG.state_dict(),\n",
    "        \"netD\": netD.state_dict(),\n",
    "        \"optimizerG\": optimizerG.state_dict(),\n",
    "        \"optimizerD\": optimizerD.state_dict(),\n",
    "    }, os.path.join(f\"{save_path}\", f\"{target_class if target_class else (\"final\" if balanced_path else \"base\")}_dcgan_final.pth\"))\n",
    "\n",
    "    return netG, netD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be787ca7",
   "metadata": {},
   "source": [
    "### Part 3.1: Training a Base DCGAN\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "319e07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Base DCGAN\n",
    "\n",
    "# trained_generator_base, trained_discriminator_base = train_dcgan(target_class = None, resume = False, checkpoint_path = None)\n",
    "\n",
    "# trained_generator_base, trained_discriminator_base = train_dcgan(target_class = None, resume = True, checkpoint_path = '../model2/gan_test/base/checkpoint_epoch_10.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6e100",
   "metadata": {},
   "source": [
    "### Part 3.2: Training a DCGAN Per Class\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DCGAN for \"cordana\" class\n",
    "# trained_generator_cordana, _ = train_dcgan(target_class = \"cordana\")\n",
    "\n",
    "# trained_generator_cordana, _ = train_dcgan(target_class = \"cordana\", resume = True, checkpoint_path = \"../model2/gan_test/cordana/checkpoint_epoch_<insert>.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe98212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DCGAN for \"healthy\" class\n",
    "# trained_generator_healthy, _ = train_dcgan(target_class = \"healthy\")\n",
    "\n",
    "# trained_generator_healthy, _ = train_dcgan(target_class = \"healthy\", resume = True, checkpoint_path = \"../model2/gan_test/healthy/checkpoint_epoch_<insert>.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DCGAN for \"pestalotiopsis\" class\n",
    "# trained_generator_pestalotiopsis, _ = train_dcgan(target_class = \"pestalotiopsis\")\n",
    "\n",
    "# trained_generator_pestalotiopsis, _ = train_dcgan(target_class = \"pestalotiopsis\", resume = True, checkpoint_path = \"../model2/gan_test/pestalotiopsis/checkpoint_epoch_<insert>.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3e960",
   "metadata": {},
   "source": [
    "## Part 4: Generating Images for Each Underrepresented Class (Cordana, Healthy, Pestalotiopsis)\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_images(dcgan_generator, amount_to_generate, class_label, output_directory):\n",
    "\n",
    "    # Set the generator to evaluation mode to disable Dropout and InstanceNorm2d updates\n",
    "    dcgan_generator.eval()\n",
    "\n",
    "    # Construct the path to the class-specific output directory\n",
    "    class_output_directory = os.path.join(output_directory, class_label)\n",
    "\n",
    "    # Create the output directory if it does not exist just in case\n",
    "    os.makedirs(class_output_directory, exist_ok = True)\n",
    "\n",
    "    # Disable gradient computation for efficiency during inference\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, amount_to_generate, 16): # Batches of 16\n",
    "            batch_size = min(16, amount_to_generate - i) # Adjusts batch size if near the end of generation\n",
    "\n",
    "            # Sample random noise vectors as generator input\n",
    "            noise = torch.randn(batch_size, INPUT_DIMENSION, 1, 1, device = DEVICE)\n",
    "\n",
    "            # Generate a batch of fake images from the noise\n",
    "            fake = dcgan_generator(noise)\n",
    "\n",
    "            # Save each generated image to the output directory\n",
    "            for j in range(batch_size):\n",
    "                save_image(\n",
    "                    fake[j], # Single image tensor\n",
    "                    os.path.join(class_output_directory, f\"gen_{i + j}.png\"),\n",
    "                    normalize = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_OUTPUT_DIRECTORY_BALANCED = \"../model2/balanced\"\n",
    "\n",
    "# Target count based on the dominant Sigatoka class\n",
    "TARGET_COUNT = 424\n",
    "\n",
    "# Dictionary of class names and their real image counts\n",
    "real_image_counts = {\n",
    "    \"cordana\"        : 145,\n",
    "    \"healthy\"        : 115,\n",
    "    \"pestalotiopsis\" : 155,\n",
    "}\n",
    "\n",
    "# Dictionary mapping class labels to their corresponding trained generators\n",
    "trained_generators = {\n",
    "    \"cordana\"        : trained_generator_cordana,\n",
    "    \"healthy\"        : trained_generator_healthy,\n",
    "    \"pestalotiopsis\" : trained_generator_pestalotiopsis,\n",
    "}\n",
    "\n",
    "# Generate synthetic images for each underrepresented class\n",
    "for label, real_count in real_image_counts.items():\n",
    "    amount_to_generate = TARGET_COUNT - real_count\n",
    "\n",
    "    generator = trained_generators[label]\n",
    "\n",
    "    generate_synthetic_images(\n",
    "        dcgan_generator = generator, amount_to_generate = amount_to_generate, class_label = label, output_directory = GAN_OUTPUT_DIRECTORY_BALANCED\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffba9e8",
   "metadata": {},
   "source": [
    "## Part 5: Training a DCGAN for feature extraction\n",
    "\n",
    "<p align=\"justify\">insert spiel</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe371d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train DCGAN on balanced training data\n",
    "\n",
    "# balanced path is path to balanced training data\n",
    "# save path of model is automatic to \"../model2/final_dcgan\"\n",
    "\n",
    "trained_generator_final, trained_discriminator_final = train_dcgan(balanced_path = GAN_OUTPUT_DIRECTORY_BALANCED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
