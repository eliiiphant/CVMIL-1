{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc82c2d",
   "metadata": {},
   "source": [
    "### Demo Code for Model 1: Self-Ensembling ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a75091",
   "metadata": {},
   "source": [
    "Import all the required libraries for setting up data, model training, and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2dd7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "import os\n",
    "import gdown\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5275daa",
   "metadata": {},
   "source": [
    "Set all necessary constants to be used in setting up the test dataset and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb04644",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"model1_vit_ensemble.pth\"\n",
    "MODEL_URL = \"https://drive.google.com/uc?id=11y4GhlkjiE2GFcVmuO-E3O3p89BawFNF\"\n",
    "\n",
    "IMAGE_PATH = \"banana_test\"\n",
    "\n",
    "NUMBER_OF_CLASSES = 4\n",
    "\n",
    "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZE_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd6eb45",
   "metadata": {},
   "source": [
    "Start by loading a non-pretrained Vision Transformer (ViT) architecture and initializing it with the weights (downloaded from Google) from the final trained model.\n",
    "\n",
    "Next, define the test images transforms: resize the input images, convert them to tensors, and normalize them, to ensure compatibility with the model’s expected input format.\n",
    "\n",
    "An inference dataset object is then defined to handle the unlabeled test images. It automatically applies the defined transforms when samples are queried.\n",
    "\n",
    "Once set up is done, a DataLoader is instantiated using this prepared dataset. The model generates predictions, which are then formatted according to the specified CSV submission requirements. Finally, the predictions are saved to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select computing device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a non-pretrained Vision Transformer (ViT), with final trained model weights.\n",
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=NUMBER_OF_CLASSES)\n",
    "gdown.download(MODEL_URL, MODEL_PATH, quiet=False)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Test images transforms: resize, convert to tensors, and normalize.\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=NORMALIZE_MEAN, \n",
    "        std=NORMALIZE_STD\n",
    "    ) \n",
    "])\n",
    "\n",
    "# Define an inference dataset object to handle unlabeled datasets. Transformed images are returned\n",
    "# when samples are queried.\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_files = sorted([\n",
    "            f for f in os.listdir(image_folder)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_name\n",
    "    \n",
    "# Create augmented dataset and loader for batch processing.\n",
    "test_dataset = InferenceDataset(IMAGE_PATH, transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "image_filenames = []\n",
    "predicted_labels = []\n",
    "labels = [\"cordana\", \"healthy\", \"pestalotiopsis\", \"sigatoka\"]\n",
    "\n",
    "# Create predictions and store them in lists\n",
    "with torch.no_grad():\n",
    "    for inputs, filenames in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        image_filenames.extend([(f\"image_{filename}\") for filename in filenames])\n",
    "        predicted_labels.extend([labels[p.item()] for p in preds])\n",
    "\n",
    "# Convert lists to dataframe and save result to a .csv file\n",
    "submission_df = pd.DataFrame({\n",
    "    \"image_filename\": image_filenames,\n",
    "    \"predicted_label\": predicted_labels\n",
    "})\n",
    "submission_df.to_csv(\"model1_output.csv\", index=False)\n",
    "\n",
    "os.remove(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25e3592",
   "metadata": {},
   "source": [
    "### Validation Code for Model 1: Self-Ensembling ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b543832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a592ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_SEED = 27\n",
    "NUMBER_OF_CLASSES = 4\n",
    "\n",
    "MODEL_PATH = \"model1_vit_ensemble.pth\"\n",
    "MODEL_URL = \"https://drive.google.com/uc?id=11y4GhlkjiE2GFcVmuO-E3O3p89BawFNF\"\n",
    "\n",
    "RAW_DATA_DIR = \"training_data\"\n",
    "\n",
    "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZE_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc39a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "base_dataset = datasets.ImageFolder(root=RAW_DATA_DIR)      \n",
    "\n",
    "# Determine subset sizes\n",
    "train_size = int(0.8*len(base_dataset))\n",
    "validation_size = len(base_dataset)-train_size\n",
    "\n",
    "# Split dataset into subsets randomly (with a pre-determined seed)\n",
    "generator = torch.Generator().manual_seed(MANUAL_SEED)\n",
    "_, validation_dataset = random_split(base_dataset, [train_size, validation_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65318dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to validation images to match the model’s input format.\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=NORMALIZE_MEAN, \n",
    "        std=NORMALIZE_STD\n",
    "    ),\n",
    "])\n",
    "\n",
    "validation_dataset = [(validation_transform(image), label) for image, label in validation_dataset]\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select computing device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a non-pretrained Vision Transformer (ViT), with final trained model weights.\n",
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=NUMBER_OF_CLASSES)\n",
    "gdown.download(MODEL_URL, MODEL_PATH, quiet=False)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Test images transforms: resize, convert to tensors, and normalize.\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=NORMALIZE_MEAN, \n",
    "        std=NORMALIZE_STD\n",
    "    ) \n",
    "])\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels_batch in validation_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "target_names = [\"cordana\", \"healthy\", \"pestalotiopsis\", \"sigatoka\"]\n",
    "print(classification_report(all_labels, all_predictions, target_names=target_names, zero_division=0))\n",
    "\n",
    "os.remove(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
